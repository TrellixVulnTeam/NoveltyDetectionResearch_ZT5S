{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang import *\n",
    "from datamodule import *\n",
    "from snli.train_utils import *\n",
    "datamodule = snli_bert_data_module(char_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ADIN_encoder_conf:\n",
    "    embedding_dim = 300\n",
    "    hidden_size = 300\n",
    "    dropout = 0.1\n",
    "    opt_labels = 3\n",
    "    attention_layer_param = 100\n",
    "    char_embedding_size = 50\n",
    "    num_layers = 1\n",
    "    \n",
    "    k = 100 # attention param\n",
    "    N = 2 # inference params\n",
    "\n",
    "    def __init__(self, lang, embedding_matrix=None, **kwargs):\n",
    "        self.embedding_matrix = None\n",
    "        self.char_emb = lang.char_emb\n",
    "        self.char_vocab_size = lang.char_vocab_size\n",
    "        self.char_word_len = lang.char_emb_max_len\n",
    "\n",
    "        if lang.tokenizer_ == \"BERT\":\n",
    "            self.vocab_size = lang.vocab_size\n",
    "            self.padding_idx = lang.bert_tokenizer.vocab[\"[PAD]\"]\n",
    "        else:\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            self.vocab_size = lang.vocab_size_final()\n",
    "            self.padding_idx = lang.word2idx[lang.config.pad]\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.conf.vocab_size,\n",
    "            embedding_dim=self.conf.embedding_dim,\n",
    "            padding_idx=self.conf.padding_idx,\n",
    "        )\n",
    "\n",
    "        if self.conf.char_emb:\n",
    "            self.char_embedding = nn.Embedding(\n",
    "                num_embeddings=self.conf.char_vocab_size,\n",
    "                embedding_dim=self.conf.char_embedding_size,\n",
    "                padding_idx=0\n",
    "            )\n",
    "            self.char_cnn = nn.Conv2d(\n",
    "                self.conf.char_word_len,\n",
    "                self.conf.char_embedding_size,\n",
    "                (1, 6),\n",
    "                stride=(1, 1),\n",
    "                padding=0,\n",
    "                bias=True\n",
    "            )\n",
    "\n",
    "        self.translate = nn.Linear(\n",
    "            self.conf.embedding_dim+(self.conf.char_embedding_size if self.conf.char_emb else 0), self.conf.hidden_size\n",
    "        )  \n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=self.conf.dropout)\n",
    "        \n",
    "        if isinstance(self.conf.embedding_matrix, np.ndarray):\n",
    "            self.embedding.from_pretrained(\n",
    "                torch.tensor(self.conf.embedding_matrix),\n",
    "                freeze=self.conf.freeze_embedding,\n",
    "            )\n",
    "\n",
    "\n",
    "    def char_embedding_forward(self,x):\n",
    "        batch_size, seq_len, char_emb_size= x.shape\n",
    "        x = x.view(-1,char_emb_size)\n",
    "        x = self.char_embedding(x)\n",
    "        x = x.view(batch_size, -1, seq_len, char_emb_size)\n",
    "        x = x.permute(0,3,2,1)\n",
    "        x = self.char_cnn(x)\n",
    "        x = torch.max(F.relu(x), 3)[0]\n",
    "        return x.view(-1,seq_len,self.conf.char_embedding_size)\n",
    "\n",
    "\n",
    "    def forward(self, inp, char_vec = None):\n",
    "        batch_size = inp.shape[0]\n",
    "        embedded = self.embedding(inp)\n",
    "        if char_vec!=None:\n",
    "            char_emb = self.char_embedding_forward(char_vec)\n",
    "            embedded = torch.cat([embedded,char_emb],dim=2)\n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded = self.translate(embedded)\n",
    "        embedded = self.dropout(self.act(embedded))\n",
    "        # embedded = embedded.permute(1, 0, 2)\n",
    "        return embedded\n",
    "        \n",
    "class InferentialModule(nn.Module):\n",
    "    def __init__(self,conf):\n",
    "        super(InferentialModule, self).__init__()\n",
    "        self.W = nn.Linear(conf.embedding_dim,conf.k,bias = False)\n",
    "        self.P = nn.Linear(conf.k,1,bias =False)\n",
    "        self.Wb = nn.Linear(4 * conf.embedding_dim,conf.embedding_dim)\n",
    "        self.LayerNorm = nn.LayerNorm(conf.embedding_dim)\n",
    "        \n",
    "    def forward(self,ha,hb):\n",
    "        e = F.softmax(self.P(F.tanh(self.W(ha*hb))))\n",
    "        hb_d = ha*e\n",
    "        hb_dd = torch.cat([hb,hb_d,hb-hb_d,hb*hb_d],dim = 2)\n",
    "        hb_b = self.LayerNorm(F.relu(self.Wb(hb_dd)))\n",
    "        return hb_b\n",
    "\n",
    "class AsyncInfer(nn.Module):\n",
    "    def __init__(self,conf):\n",
    "        super(AsyncInfer, self).__init__()\n",
    "        self.inf1 = InferentialModule(conf)\n",
    "        self.inf2 = InferentialModule(conf)\n",
    "        self.lstm_layer1 = nn.LSTM(\n",
    "            input_size=int(2*conf.hidden_size),\n",
    "            hidden_size=int(conf.hidden_size/2),\n",
    "            num_layers=conf.num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.lstm_layer2 = nn.LSTM(\n",
    "            input_size=int(2*conf.hidden_size),\n",
    "            hidden_size=int(conf.hidden_size/2),\n",
    "            num_layers=conf.num_layers,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "    \n",
    "    def forward(self,Vp,Vq):\n",
    "        vq_hat = self.inf1(Vp, Vq)\n",
    "        vp_hat = self.inf2(vq_hat, Vp)\n",
    "        vq_d = torch.cat([Vq, vq_hat],dim=2)\n",
    "        vp_d = torch.cat([Vp, vp_hat],dim=2)\n",
    "        Vq_new,(_,_) = self.lstm_layer1(vq_d)\n",
    "        Vp_new,(_,_) = self.lstm_layer1(vp_d)\n",
    "        return Vp_new,Vq_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ADIN(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(ADIN, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.encoder = SentenceEncoder(conf)\n",
    "        self.inference_modules = nn.ModuleList([AsyncInfer(conf) for i in range(conf.N)])\n",
    "        self.r = nn.Linear(8*conf.hidden_size,conf.hidden_size)\n",
    "        self.v = nn.Linear(conf.hidden_size,3)\n",
    "        self.dropout = nn.Dropout(p=self.conf.dropout)\n",
    "        \n",
    "    def forward(self, x0, x1, x0_char_vec = None, x1_char_vec = None):\n",
    "        x0_enc = self.encoder(x0.long(),char_vec = x0_char_vec)\n",
    "        x0_enc = self.dropout(x0_enc)\n",
    "        x1_enc = self.encoder(x1.long(),char_vec = x1_char_vec)\n",
    "        x1_enc = self.dropout(x1_enc)\n",
    "\n",
    "        for inf_module in self.inference_modules:\n",
    "            x0_enc,x1_enc = inf_module(x0_enc,x1_enc)\n",
    "        \n",
    "        x0_mean = torch.mean(x0_enc,dim=1)\n",
    "        x1_mean = torch.mean(x1_enc,dim=1)\n",
    "\n",
    "        x0_max = torch.max(x0_enc,dim=1)[0]\n",
    "        x1_max = torch.max(x1_enc,dim=1)[0]\n",
    "\n",
    "        x0_new = torch.cat([x0_mean,x0_max],dim=1)\n",
    "        x1_new = torch.cat([x1_mean,x1_max],dim=1)\n",
    "\n",
    "        r = torch.cat([x0_new,x1_new,x0_new-x1_new,x0_new*x1_new],dim=1)\n",
    "        v = F.relu(self.r(r))\n",
    "        y = F.softmax(self.v(v))\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hparams = {\n",
    "    \"optimizer_base\": {\n",
    "        \"optim\": \"adamw\",\n",
    "        \"lr\": 0.0010039910781394373,\n",
    "        \"scheduler\": \"const\",\n",
    "    },\n",
    "    \"optimizer_tune\": {\n",
    "        \"optim\": \"adam\",\n",
    "        \"lr\": 0.0010039910781394373,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"scheduler\": \"lambda\",\n",
    "    },\n",
    "    \"switch_epoch\": 5,\n",
    "}\n",
    "\n",
    "lang = datamodule.Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = ADIN_encoder_conf(lang, None)\n",
    "model = ADIN(model_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datamodule.train_dataloader():\n",
    "    a,b,c,d,e = i\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 100, 300])\n",
      "torch.Size([128, 100, 300])\n",
      "torch.Size([128, 2400])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([128, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model(a,c,b,d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}