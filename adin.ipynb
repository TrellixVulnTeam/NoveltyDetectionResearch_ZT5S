{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang import *\n",
    "from datamodule import *\n",
    "from snli.train_utils import *\n",
    "datamodule = snli_bert_data_module(char_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ADIN_encoder_conf:\n",
    "    embedding_dim = 300\n",
    "    hidden_size = 300\n",
    "    dropout = 0.1\n",
    "    opt_labels = 3\n",
    "    attention_layer_param = 100\n",
    "    char_embedding_size = 50\n",
    "\n",
    "    s = 100 # attention param\n",
    "\n",
    "    def __init__(self, lang, embedding_matrix=None, **kwargs):\n",
    "        self.embedding_matrix = None\n",
    "        self.char_emb = lang.char_emb\n",
    "        self.char_vocab_size = lang.char_vocab_size\n",
    "        self.char_word_len = lang.char_emb_max_len\n",
    "\n",
    "        if lang.tokenizer_ == \"BERT\":\n",
    "            self.vocab_size = lang.vocab_size\n",
    "            self.padding_idx = lang.bert_tokenizer.vocab[\"[PAD]\"]\n",
    "        else:\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            self.vocab_size = lang.vocab_size_final()\n",
    "            self.padding_idx = lang.word2idx[lang.config.pad]\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(Attn_Encoder, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.conf.vocab_size,\n",
    "            embedding_dim=self.conf.embedding_dim,\n",
    "            padding_idx=self.conf.padding_idx,\n",
    "        )\n",
    "\n",
    "        if self.conf.char_emb:\n",
    "            self.char_embedding = nn.Embedding(\n",
    "                num_embeddings=self.conf.char_vocab_size,\n",
    "                embedding_dim=self.conf.char_embedding_size,\n",
    "                padding_idx=0\n",
    "            )\n",
    "            self.char_cnn = nn.Conv2d(\n",
    "                self.conf.char_word_len,\n",
    "                self.conf.char_embedding_size,\n",
    "                (1, 6),\n",
    "                stride=(1, 1),\n",
    "                padding=0,\n",
    "                bias=True\n",
    "            )\n",
    "\n",
    "        self.translate = nn.Linear(\n",
    "            self.conf.embedding_dim+(self.conf.char_embedding_size if self.conf.char_emb else 0), self.conf.hidden_size\n",
    "        )  \n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        if isinstance(self.conf.embedding_matrix, np.ndarray):\n",
    "            self.embedding.from_pretrained(\n",
    "                torch.tensor(self.conf.embedding_matrix),\n",
    "                freeze=self.conf.freeze_embedding,\n",
    "            )\n",
    "\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size=self.conf.hidden_size,\n",
    "            hidden_size=self.conf.hidden_size,\n",
    "            num_layers=self.conf.num_layers,\n",
    "            bidirectional=self.conf.bidirectional,\n",
    "        )\n",
    "\n",
    "    def char_embedding_forward(self,x):\n",
    "        #X - [batch_size, seq_len, char_emb_size])\n",
    "        batch_size, seq_len, char_emb_size= x.shape\n",
    "        x = x.view(-1,char_emb_size)\n",
    "        x = self.char_embedding(x) #(batch_size * seq_len, char_emb_size, emb_size)\n",
    "        x = x.view(batch_size, -1, seq_len, char_emb_size)\n",
    "        x = x.permute(0,3,2,1)\n",
    "        x = self.char_cnn(x)\n",
    "        x = torch.max(F.relu(x), 3)[0]\n",
    "        return x.view(-1,seq_len,self.conf.char_embedding_size)\n",
    "\n",
    "\n",
    "    def forward(self, inp, char_vec = None):\n",
    "        batch_size = inp.shape[0]\n",
    "        embedded = self.embedding(inp)\n",
    "        if char_vec!=None:\n",
    "            char_emb = self.char_embedding_forward(char_vec)\n",
    "            embedded = torch.cat([embedded,char_emb],dim=2)\n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded = self.translate(embedded)\n",
    "        embedded = self.dropout(self.act(embedded))\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        all_, (hid, cell) = self.lstm_layer(embedded)\n",
    "        return all_\n",
    "        \n",
    "class InferentialModule(nn.Module):\n",
    "    def __init__(self,conf):\n",
    "        self.W = nn.Linear(conf.embedding_dim,conf.k,bias = False)\n",
    "        self.P = nn.Linear(conf.k,1,bias =False)\n",
    "        self.Wb = nn.Linear(4 * conf.embedding_dim,conf.embedding_dim)\n",
    "        self.LayerNorm = nn.LayerNorm(conf.embedding_dim)\n",
    "        \n",
    "    def forward(self,ha,hb):\n",
    "        e = F.softmax(self.P(F.tanh(self.W(ha*hb))))\n",
    "        hb_d = ha*e\n",
    "        hb_dd = torch.cat([hb,hb_d,hb-hb_d,hb*hb_d],dim = 2)\n",
    "        hb_b = self.LayerNorm(F.ReLu(self.Wb(hb_dd)))\n",
    "        return hb_b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ADIN(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(Attn_encoder_snli, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.encoder = SentenceEncoder(conf)\n",
    "        self.inference_module = InferentialModule(conf)\n",
    "        \n",
    "\n",
    "    def forward(self, x0, x1, x0_char_vec = None, x1_char_vec = None):\n",
    "        x0_enc = self.encoder(x0.long(),char_vec = x0_char_vec)\n",
    "        x0_enc = self.dropout(x0_enc)\n",
    "        x1_enc = self.encoder(x1.long(),char_vec = x1_char_vec)\n",
    "        x1_enc = self.dropout(x1_enc)\n",
    "        cont = torch.cat(\n",
    "            [x0_enc, x1_enc, torch.abs(x0_enc - x1_enc), x0_enc * x1_enc], dim=2\n",
    "        )\n",
    "        opt = self.fc_in(cont)\n",
    "        opt = self.dropout(opt)\n",
    "        for fc in self.fcs:\n",
    "            opt = fc(opt)\n",
    "            opt = self.dropout(opt)\n",
    "            opt = self.act(opt)\n",
    "        opt = self.fc_out(opt)\n",
    "        return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hparams = {\n",
    "    \"optimizer_base\": {\n",
    "        \"optim\": \"adamw\",\n",
    "        \"lr\": 0.0010039910781394373,\n",
    "        \"scheduler\": \"const\",\n",
    "    },\n",
    "    \"optimizer_tune\": {\n",
    "        \"optim\": \"adam\",\n",
    "        \"lr\": 0.0010039910781394373,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"scheduler\": \"lambda\",\n",
    "    },\n",
    "    \"switch_epoch\": 5,\n",
    "}\n",
    "\n",
    "lang = datamodule.Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = Attn_Encoder_conf(lang,None, **conf_kwargs)\n",
    "model = SNLI_char_emb(Attn_encoder_snli, model_conf, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha = torch.rand((32,100,300))\n",
    "hb = torch.rand((32,100,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 300])"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "(ha*hb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nn.Linear(300,50,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 50])"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "W(ha*hb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = nn.Linear(50,1,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = F.softmax( P(F.tanh(W(ha*hb))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_d = (ha * e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_dd = torch.cat([hb,hb_d,hb-hb_d,hb*hb_d],dim = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 1200])"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "hb_dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = nn.Linear(1200,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 300])"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "wb(hb_dd).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = nn.LayerNorm(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 300])"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "ln(wb(hb_dd)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}