{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang import *\n",
    "from datamodule import *\n",
    "from snli.train_utils import *\n",
    "datamodule = snli_bert_data_module(char_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ADIN_encoder_conf:\n",
    "    embedding_dim = 300\n",
    "    hidden_size = 300\n",
    "    dropout = 0.1\n",
    "    opt_labels = 3\n",
    "    attention_layer_param = 100\n",
    "    activation = \"tanh\"\n",
    "    char_embedding_size = 50\n",
    "\n",
    "    def __init__(self, lang, embedding_matrix=None, **kwargs):\n",
    "        self.embedding_matrix = None\n",
    "        self.char_emb = lang.char_emb\n",
    "        self.char_vocab_size = lang.char_vocab_size\n",
    "        self.char_word_len = lang.char_emb_max_len\n",
    "\n",
    "        if lang.tokenizer_ == \"BERT\":\n",
    "            self.vocab_size = lang.vocab_size\n",
    "            self.padding_idx = lang.bert_tokenizer.vocab[\"[PAD]\"]\n",
    "        else:\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            self.vocab_size = lang.vocab_size_final()\n",
    "            self.padding_idx = lang.word2idx[lang.config.pad]\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(Attn_Encoder, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.conf.vocab_size,\n",
    "            embedding_dim=self.conf.embedding_dim,\n",
    "            padding_idx=self.conf.padding_idx,\n",
    "        )\n",
    "\n",
    "        if self.conf.char_emb:\n",
    "            self.char_embedding = nn.Embedding(\n",
    "                num_embeddings=self.conf.char_vocab_size,\n",
    "                embedding_dim=self.conf.char_embedding_size,\n",
    "                padding_idx=0\n",
    "            )\n",
    "            self.char_cnn = nn.Conv2d(\n",
    "                self.conf.char_word_len,\n",
    "                self.conf.char_embedding_size,\n",
    "                (1, 6),\n",
    "                stride=(1, 1),\n",
    "                padding=0,\n",
    "                bias=True\n",
    "            )\n",
    "\n",
    "        self.translate = nn.Linear(\n",
    "            self.conf.embedding_dim+(self.conf.char_embedding_size if self.conf.char_emb else 0), self.conf.hidden_size\n",
    "        )  \n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        if isinstance(self.conf.embedding_matrix, np.ndarray):\n",
    "            self.embedding.from_pretrained(\n",
    "                torch.tensor(self.conf.embedding_matrix),\n",
    "                freeze=self.conf.freeze_embedding,\n",
    "            )\n",
    "\n",
    "    def char_embedding_forward(self,x):\n",
    "        #X - [batch_size, seq_len, char_emb_size])\n",
    "        batch_size, seq_len, char_emb_size= x.shape\n",
    "        x = x.view(-1,char_emb_size)\n",
    "        x = self.char_embedding(x) #(batch_size * seq_len, char_emb_size, emb_size)\n",
    "        x = x.view(batch_size, -1, seq_len, char_emb_size)\n",
    "        x = x.permute(0,3,2,1)\n",
    "        x = self.char_cnn(x)\n",
    "        x = torch.max(F.relu(x), 3)[0]\n",
    "        return x.view(-1,seq_len,self.conf.char_embedding_size)\n",
    "\n",
    "\n",
    "    def forward(self, inp, char_vec = None):\n",
    "        batch_size = inp.shape[0]\n",
    "        embedded = self.embedding(inp)\n",
    "        if char_vec!=None:\n",
    "            char_emb = self.char_embedding_forward(char_vec)\n",
    "            embedded = torch.cat([embedded,char_emb],dim=2)\n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded = self.translate(embedded)\n",
    "        embedded = self.dropout(self.act(embedded))\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "\n",
    "\n",
    "class ADIN(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(Attn_encoder_snli, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.encoder = SentenceEncoder(conf)\n",
    "        self.fc_in = nn.Linear(\n",
    "            (2 if conf.bidirectional else 1) * 4 * self.conf.hidden_size,\n",
    "            self.conf.hidden_size,\n",
    "        )\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(self.conf.hidden_size, self.conf.hidden_size)\n",
    "                for i in range(self.conf.fcs)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(self.conf.hidden_size, self.conf.opt_labels)\n",
    "        if self.conf.activation.lower() == \"relu\".lower():\n",
    "            self.act = nn.ReLU()\n",
    "        elif self.conf.activation.lower() == \"tanh\".lower():\n",
    "            self.act = nn.Tanh()\n",
    "        elif self.conf.activation.lower() == \"leakyrelu\".lower():\n",
    "            self.act = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.dropout = nn.Dropout(p=self.conf.dropout)\n",
    "\n",
    "    def forward(self, x0, x1, x0_char_vec = None, x1_char_vec = None):\n",
    "        x0_enc = self.encoder(x0.long(),char_vec = x0_char_vec)\n",
    "        x0_enc = self.dropout(x0_enc)\n",
    "        x1_enc = self.encoder(x1.long(),char_vec = x1_char_vec)\n",
    "        x1_enc = self.dropout(x1_enc)\n",
    "        cont = torch.cat(\n",
    "            [x0_enc, x1_enc, torch.abs(x0_enc - x1_enc), x0_enc * x1_enc], dim=2\n",
    "        )\n",
    "        opt = self.fc_in(cont)\n",
    "        opt = self.dropout(opt)\n",
    "        for fc in self.fcs:\n",
    "            opt = fc(opt)\n",
    "            opt = self.dropout(opt)\n",
    "            opt = self.act(opt)\n",
    "        opt = self.fc_out(opt)\n",
    "        return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hparams = {\n",
    "    \"optimizer_base\": {\n",
    "        \"optim\": \"adamw\",\n",
    "        \"lr\": 0.0010039910781394373,\n",
    "        \"scheduler\": \"const\",\n",
    "    },\n",
    "    \"optimizer_tune\": {\n",
    "        \"optim\": \"adam\",\n",
    "        \"lr\": 0.0010039910781394373,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"scheduler\": \"lambda\",\n",
    "    },\n",
    "    \"switch_epoch\": 5,\n",
    "}\n",
    "\n",
    "lang = datamodule.Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = Attn_Encoder_conf(lang,None, **conf_kwargs)\n",
    "model = SNLI_char_emb(Attn_encoder_snli, model_conf, hparams)"
   ]
  }
 ]
}