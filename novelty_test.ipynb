{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.novelty import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<src.datasets.novelty.DLND at 0x7f53e5667b00>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./temp.json','w') as f:\n",
    "    f.writelines([json.dumps(i)+\"\\n\" for i in d[3:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"NLI\"\n",
    "NEPTUNE_API = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiMTg3MzU5NjQtMmIxZC00Njg0LTgzYzMtN2UwYjVlYzVhNDg5In0=\"\n",
    "import neptune\n",
    "project = neptune.init(f\"aparkhi/{project}\", api_token=NEPTUNE_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.defaults import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_models_from_neptune(\"NLI-81\")\n",
    "field = load_field('NLI-81')\n",
    "# field = None\n",
    "\n",
    "\n",
    "dataset_conf = {'dataset': 'dlnd', 'max_num_sent': 50,\"sent_tokenizer\":\"spacy\",\"batch_size\":32,\"device\":\"cuda\"}\n",
    "# dataset_conf = {'dataset': 'dlnd', 'max_num_sent': 50,\"sent_tokenizer\":\"spacy\", \"tokenizer\":'spacy',\"max_len\":50,\"batch_size\":32,\"device\":\"cuda\"}\n",
    "model_conf = {'results_dir': 'results', 'device': 'cuda', 'dropout': 0.3, 'dataset': 'dlnd', 'hidden_size': 400, 'use_glove': False, \"max_num_sent\": 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "src.datasets.novelty.DLND\n.data/dlnd\n"
     ]
    }
   ],
   "source": [
    "from src.datasets.novelty import *\n",
    "data = dlnd(dataset_conf,sentence_field = field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n[torchtext.data.batch.Batch of size 32]\n\t[.source]:[torch.cuda.LongTensor of size 32x50x50 (GPU 0)]\n\t[.target]:[torch.cuda.LongTensor of size 32x50x50 (GPU 0)]\n\t[.label]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n"
     ]
    }
   ],
   "source": [
    "for i in data.train_iter:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class DAN(nn.Module):\n",
    "    def __init__(self,conf,encoder):\n",
    "        super(DAN,self).__init__()\n",
    "        self.conf = conf\n",
    "        self.num_sent = conf[\"max_num_sent\"]\n",
    "        self.encoder = encoder\n",
    "        self.translate = nn.Linear(2 * self.conf[\"encoder_dim\"], self.conf[\"hidden_size\"])\n",
    "        self.template = nn.Parameter(torch.zeros((1)), requires_grad=True)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(conf[\"dropout\"])\n",
    "\n",
    "        self.mlp_f = nn.Linear(self.conf[\"hidden_size\"], self.conf[\"hidden_size\"])\n",
    "        self.mlp_g = nn.Linear(2*self.conf[\"hidden_size\"], self.conf[\"hidden_size\"])\n",
    "        self.mlp_h = nn.Linear(2*self.conf[\"hidden_size\"], self.conf[\"hidden_size\"])\n",
    "        self.linear = nn.Linear(self.conf[\"hidden_size\"],2)\n",
    "\n",
    "    def encode_sent(self,inp):\n",
    "        batch_size,_,_ = inp.shape\n",
    "        x = inp.view(-1,self.sent_len)\n",
    "\n",
    "        x_padded_idx = x.sum(dim=1) != 0    \n",
    "        x_enc = []\n",
    "        for sub_batch in x[x_padded_idx].split(64):\n",
    "            x_enc.append(self.encoder(sub_batch)[0])\n",
    "        x_enc = torch.cat(x_enc, dim=0)\n",
    "\n",
    "        x_enc_t = torch.zeros((batch_size * self.num_sent, x_enc.size(1))).to(\n",
    "            self.template.device\n",
    "        )\n",
    "\n",
    "        x_enc_t[x_padded_idx] = x_enc\n",
    "        x_enc_t = x_enc_t.view(batch_size, self.num_sent, -1)\n",
    "    \n",
    "        embedded = self.dropout(self.translate(x_enc_t))\n",
    "        embedded = self.act(embedded)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        return embedded\n",
    "\n",
    "\n",
    "    def forward(self,x0,x1):\n",
    "        x0_enc = self.encode_sent(x0).permute(1,0,2)\n",
    "        x1_enc = self.encode_sent(x1).permute(1,0,2)\n",
    "\n",
    "        f1 = self.act(self.dropout(self.mlp_f(x0_enc)))\n",
    "        f2 = self.act(self.dropout(self.mlp_f(x1_enc)))\n",
    "\n",
    "        score1 = torch.bmm(f1, torch.transpose(f2, 1, 2))\n",
    "        prob1 = F.softmax(score1.view(-1, self.num_sent)).view(-1, self.num_sent, self.num_sent)\n",
    "\n",
    "        score2 = torch.transpose(score1.contiguous(), 1, 2)\n",
    "        score2 = score2.contiguous()\n",
    "\n",
    "        prob2 = F.softmax(score2.view(-1, self.num_sent)).view(-1, self.num_sent, self.num_sent)\n",
    "\n",
    "        sent1_combine = torch.cat((x0_enc, torch.bmm(prob1, x1_enc)), 2)\n",
    "        sent2_combine = torch.cat((x1_enc, torch.bmm(prob2, x0_enc)), 2)\n",
    "\n",
    "        \n",
    "\n",
    "        g1 = self.act(self.dropout(self.mlp_g(sent1_combine)))\n",
    "        g2 = self.act(self.dropout(self.mlp_g(sent2_combine)))\n",
    "\n",
    "        sent1_output = torch.sum(g1, 1)  \n",
    "        sent1_output = torch.squeeze(sent1_output, 1)\n",
    "    \n",
    "        sent2_output = torch.sum(g2, 1)  \n",
    "        sent2_output = torch.squeeze(sent2_output, 1)\n",
    "\n",
    "\n",
    "        input_combine = torch.cat((sent1_output * sent2_output, torch.abs(sent1_output - sent2_output)), 1)\n",
    "        \n",
    "        h = self.act(self.dropout(self.mlp_h(input_combine)))\n",
    "        opt = self.linear(h)\n",
    "        return opt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DAN(\n",
       "  (encoder): AttnBiLSTM_snli(\n",
       "    (encoder): Attn_Encoder(\n",
       "      (embedding): Embedding(33934, 300)\n",
       "      (translate): Linear(in_features=300, out_features=400, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (lstm_layer): LSTM(400, 400, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "      (attention): Attention(\n",
       "        (Ws): Linear(in_features=800, out_features=200, bias=False)\n",
       "        (Wa): Linear(in_features=200, out_features=1, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_in): Linear(in_features=3200, out_features=400, bias=True)\n",
       "    (fcs): ModuleList(\n",
       "      (0): Linear(in_features=400, out_features=400, bias=True)\n",
       "    )\n",
       "    (fc_out): Linear(in_features=400, out_features=3, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (softmax): Softmax(dim=2)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (translate): Linear(in_features=800, out_features=400, bias=True)\n",
       "  (act): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (mlp_f): Linear(in_features=400, out_features=400, bias=True)\n",
       "  (mlp_g): Linear(in_features=800, out_features=400, bias=True)\n",
       "  (mlp_h): Linear(in_features=800, out_features=400, bias=True)\n",
       "  (linear): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "DAN(model_conf,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder(_id):\n",
    "    model_path = os.path.join('./results/',_id,\"model.pt\")\n",
    "    model_data = torch.load(model_path)\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = load_encoder('NLI-81')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.nli_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attn_bilstm_snli(model_data['options'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf[\"encoder_dim\"] = model_data[\"options\"][\"hidden_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder1(enc_data):\n",
    "    if enc_data['options'].get(\"attention_layer_param\",0)==0:\n",
    "        model = bilstm_snli(enc_data[\"options\"])\n",
    "    elif enc_data['options'].get(\"r\",0)==0:\n",
    "        model = attn_bilstm_snli(enc_data[\"options\"])\n",
    "    else:\n",
    "        model = struc_attn_snli(enc_data[\"options\"])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 81.19284698232066,\n",
       " 'dataset_conf': {'batch_size': 128,\n",
       "  'dataset': 'snli',\n",
       "  'device': 'cuda',\n",
       "  'eos_token': '<eos>',\n",
       "  'init_token': '<sos>',\n",
       "  'lower': True,\n",
       "  'max_len': 50,\n",
       "  'max_word_len': 10,\n",
       "  'pad_token': '<pad>',\n",
       "  'preprocessing': None,\n",
       "  'tokenize': 'spacy',\n",
       "  'tokenizer': 'spacy',\n",
       "  'unk_token': '<unk>',\n",
       "  'use_char_emb': False,\n",
       "  'use_vocab': True},\n",
       " 'model_dict': OrderedDict([('encoder.embedding.weight',\n",
       "               tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       ...,\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                       [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')),\n",
       "              ('encoder.translate.weight',\n",
       "               tensor([[-0.1381,  0.0271, -0.0114,  ..., -0.1154, -0.1232, -0.0280],\n",
       "                       [-0.1849,  0.0321, -0.0733,  ...,  0.0817, -0.1482,  0.0119],\n",
       "                       [ 0.0831, -0.0198,  0.1953,  ...,  0.0828,  0.0440,  0.0380],\n",
       "                       ...,\n",
       "                       [-0.0955,  0.1087,  0.0268,  ..., -0.0456,  0.0157,  0.0101],\n",
       "                       [-0.0728,  0.1277,  0.0075,  ...,  0.0364, -0.0725, -0.1840],\n",
       "                       [ 0.0367, -0.1331,  0.2491,  ...,  0.1645, -0.0367, -0.1516]],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.translate.bias',\n",
       "               tensor([-0.0181, -0.3079, -0.2290, -0.2587, -0.1152, -0.3625, -0.0882, -0.1845,\n",
       "                       -0.2370, -0.1587, -0.0851, -0.1423, -0.1297, -0.0483, -0.1001, -0.2337,\n",
       "                       -0.1005, -0.1399, -0.3066, -0.0756, -0.1005, -0.0900, -0.2865, -0.1064,\n",
       "                       -0.1411,  0.3861, -0.1784, -0.0919, -0.1788, -0.2466, -0.0370, -0.0990,\n",
       "                       -0.3486,  0.1829, -0.2449, -0.1434, -0.1543, -0.2357, -0.0539, -0.2761,\n",
       "                       -0.2732, -0.1189, -0.0034, -0.1740, -0.2965, -0.3636,  0.1932, -0.1014,\n",
       "                       -0.0918, -0.1713, -0.0876, -0.2133, -0.2774, -0.0669, -0.2861, -0.1014,\n",
       "                       -0.1314, -0.0953, -0.0713, -0.1398, -0.2933, -0.0187,  0.1103, -0.0876,\n",
       "                       -0.0814, -0.1355, -0.0651, -0.1778, -0.1514, -0.4289, -0.2364, -0.1007,\n",
       "                       -0.2541, -0.1051, -0.0649, -0.1906, -0.0985, -0.1591, -0.1214, -0.1036,\n",
       "                       -0.0801, -0.0678, -0.1467, -0.1307, -0.1788, -0.2157, -0.0833, -0.0826,\n",
       "                       -0.0838, -0.0899, -0.3888, -0.1684, -0.1982, -0.1199, -0.2640, -0.1862,\n",
       "                       -0.2995, -0.3347, -0.0417, -0.4555,  0.2512, -0.2974,  0.1566, -0.0451,\n",
       "                       -0.3355, -0.0875, -0.1058, -0.0569, -0.1485, -0.1707, -0.1450, -0.0134,\n",
       "                       -0.1463, -0.1531, -0.0368, -0.2467, -0.2400, -0.1571, -0.1637, -0.2431,\n",
       "                       -0.2209, -0.1613, -0.0526, -0.1724, -0.2686, -0.2313, -0.2751, -0.1066,\n",
       "                        0.1514, -0.2512, -0.1391, -0.1377, -0.1189, -0.2233, -0.0573, -0.1657,\n",
       "                       -0.0957, -0.1734, -0.1897, -0.0440,  0.1138, -0.2204, -0.2324, -0.1150,\n",
       "                       -0.1501, -0.1744,  0.4448, -0.2738, -0.2350, -0.3381, -0.1337, -0.1746,\n",
       "                       -0.3833, -0.1565, -0.1964, -0.1387, -0.0730, -0.2120, -0.0463, -0.1176,\n",
       "                       -0.0594, -0.1692, -0.1341,  0.2198, -0.3106, -0.2324, -0.1136, -0.1765,\n",
       "                       -0.0692, -0.2527, -0.1733, -0.2227, -0.3201,  0.3975, -0.2238, -0.3464,\n",
       "                       -0.1812, -0.3901, -0.1776, -0.2731, -0.0717, -0.2703, -0.0753, -0.0873,\n",
       "                       -0.0294, -0.1295, -0.1702, -0.1820, -0.0591,  0.2084, -0.0796, -0.0813,\n",
       "                       -0.2754, -0.2228, -0.1313, -0.1906, -0.3007, -0.1397, -0.1006, -0.0741,\n",
       "                       -0.0484, -0.0709, -0.0250, -0.1591, -0.0615, -0.0601, -0.0774,  0.1030,\n",
       "                       -0.2547, -0.3547, -0.2004, -0.0385, -0.2881, -0.2507, -0.0709, -0.1819,\n",
       "                       -0.0966,  0.0380, -0.0775, -0.0865, -0.1076, -0.2265, -0.1907, -0.0470,\n",
       "                       -0.1214, -0.3077, -0.2343, -0.1411, -0.0454, -0.1710, -0.0303, -0.1268,\n",
       "                       -0.2081, -0.2686, -0.1117, -0.1242, -0.1347, -0.0909, -0.3564, -0.0281,\n",
       "                       -0.1155, -0.1697, -0.0591, -0.0986, -0.0603, -0.1388, -0.2831, -0.0920,\n",
       "                       -0.1297, -0.0218, -0.0894, -0.1330, -0.0638, -0.0539, -0.1458, -0.1828,\n",
       "                       -0.2555, -0.1741, -0.1487, -0.1955, -0.2879, -0.0695, -0.1410, -0.4066,\n",
       "                       -0.1243, -0.1528, -0.0915, -0.3737, -0.1626, -0.1575, -0.0409, -0.0958,\n",
       "                       -0.0702, -0.1706, -0.0395, -0.0602, -0.0247, -0.1872, -0.0966, -0.1402,\n",
       "                       -0.1259, -0.3070,  0.4180, -0.1216,  0.2337, -0.0829,  0.0597, -0.1697,\n",
       "                       -0.3114, -0.2311, -0.1142, -0.1040, -0.2811, -0.1531, -0.1045, -0.0411,\n",
       "                       -0.0765, -0.1776, -0.0430, -0.0884, -0.1456, -0.0576, -0.1299, -0.1862,\n",
       "                       -0.1686, -0.1265, -0.0766, -0.1429, -0.1377, -0.0671, -0.1498, -0.0737,\n",
       "                        0.3775, -0.1529, -0.1674, -0.2678, -0.0984, -0.0897, -0.1654, -0.1274,\n",
       "                       -0.0511, -0.1618,  0.2729, -0.2352, -0.0708, -0.1678, -0.3023, -0.1945,\n",
       "                       -0.2256, -0.0422, -0.3514, -0.1250, -0.0777, -0.0321, -0.0924, -0.0681,\n",
       "                       -0.1759, -0.3839, -0.0842, -0.1338, -0.0945, -0.3975, -0.2061, -0.1604,\n",
       "                       -0.1257, -0.1388, -0.0318, -0.0865, -0.1673, -0.2086, -0.1867, -0.2151,\n",
       "                       -0.3400, -0.2113, -0.0191, -0.1664, -0.1177, -0.1803, -0.3727, -0.3398,\n",
       "                       -0.0779, -0.1987, -0.2412, -0.2720, -0.2981, -0.1445, -0.0437, -0.2033,\n",
       "                       -0.0849, -0.1387, -0.2705, -0.2012, -0.1448, -0.1196, -0.0243, -0.0337,\n",
       "                       -0.2137, -0.0758, -0.2894, -0.1309, -0.1281, -0.1627, -0.1178, -0.0749,\n",
       "                       -0.0927, -0.1229, -0.0752, -0.1120, -0.1622, -0.1859, -0.0865, -0.1219,\n",
       "                       -0.0245, -0.2403, -0.1289, -0.0985, -0.1875, -0.1443, -0.2248, -0.0726],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.lstm_layer.weight_ih_l0',\n",
       "               tensor([[-0.0453,  0.0041, -0.0302,  ...,  0.0255,  0.0281,  0.0142],\n",
       "                       [ 0.0284,  0.1185, -0.0879,  ..., -0.0257,  0.0030,  0.1034],\n",
       "                       [ 0.0073, -0.0065,  0.0138,  ...,  0.0426,  0.0388, -0.0438],\n",
       "                       ...,\n",
       "                       [ 0.0352,  0.0098, -0.0375,  ..., -0.0063, -0.0171,  0.0486],\n",
       "                       [ 0.0242, -0.0630, -0.0497,  ...,  0.0038, -0.0594,  0.0190],\n",
       "                       [-0.0104, -0.0543, -0.0359,  ...,  0.0683, -0.0181, -0.0111]],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.lstm_layer.weight_hh_l0',\n",
       "               tensor([[ 0.0273, -0.0281, -0.0490,  ..., -0.0316,  0.0315, -0.0063],\n",
       "                       [ 0.0108, -0.0707, -0.0561,  ..., -0.0333,  0.0306,  0.0550],\n",
       "                       [-0.0423,  0.0249, -0.0121,  ..., -0.0507,  0.0384, -0.0230],\n",
       "                       ...,\n",
       "                       [-0.0156,  0.0194, -0.0540,  ...,  0.0233, -0.0196, -0.0155],\n",
       "                       [-0.0081, -0.0574, -0.0291,  ...,  0.0439, -0.0207, -0.0450],\n",
       "                       [-0.0409,  0.0373, -0.0458,  ..., -0.0627,  0.0104, -0.0055]],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.lstm_layer.bias_ih_l0',\n",
       "               tensor([-0.1064, -0.1085, -0.1277,  ..., -0.0695, -0.0570, -0.1305],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.lstm_layer.bias_hh_l0',\n",
       "               tensor([-0.1013, -0.0628, -0.0614,  ..., -0.1391, -0.0521, -0.0526],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.lstm_layer.weight_ih_l0_reverse',\n",
       "               tensor([[ 0.0403,  0.0154,  0.0316,  ...,  0.0310,  0.0234, -0.0400],\n",
       "                       [-0.0054,  0.0260,  0.0281,  ...,  0.0447,  0.0119,  0.0164],\n",
       "                       [ 0.0398, -0.0646,  0.1129,  ...,  0.0577, -0.0978,  0.0521],\n",
       "                       ...,\n",
       "                       [-0.0314, -0.0213, -0.0555,  ...,  0.0137,  0.0107, -0.0238],\n",
       "                       [-0.0433, -0.0689,  0.0248,  ...,  0.0148, -0.0498,  0.0230],\n",
       "                       [-0.0098,  0.0305,  0.0046,  ..., -0.0433,  0.0126, -0.0424]],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.lstm_layer.weight_hh_l0_reverse',\n",
       "               tensor([[ 0.0360,  0.0450,  0.0348,  ..., -0.0101, -0.0490,  0.0335],\n",
       "                       [ 0.0005, -0.0326, -0.0244,  ..., -0.0188, -0.0124,  0.0145],\n",
       "                       [ 0.0027,  0.0113,  0.0547,  ..., -0.0002, -0.0611, -0.0296],\n",
       "                       ...,\n",
       "                       [ 0.0164,  0.0381,  0.0266,  ..., -0.0531, -0.0120, -0.0305],\n",
       "                       [-0.0200,  0.0114, -0.0075,  ...,  0.0058,  0.0248,  0.0177],\n",
       "                       [ 0.0049, -0.0026, -0.0083,  ..., -0.0176,  0.0270, -0.0476]],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.lstm_layer.bias_ih_l0_reverse',\n",
       "               tensor([-0.1122, -0.1372, -0.5016,  ..., -0.1119, -0.1107, -0.1085],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.lstm_layer.bias_hh_l0_reverse',\n",
       "               tensor([-0.0736, -0.1035, -0.5401,  ..., -0.0347, -0.1400, -0.0725],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.attention.Ws.weight',\n",
       "               tensor([[-0.0046,  0.0053, -0.0136,  ...,  0.0196, -0.0283, -0.0227],\n",
       "                       [ 0.0182,  0.0051,  0.0283,  ..., -0.0322, -0.0095,  0.0298],\n",
       "                       [-0.0086, -0.0145,  0.0266,  ..., -0.0284,  0.0091,  0.0054],\n",
       "                       ...,\n",
       "                       [ 0.0239,  0.0018,  0.0287,  ..., -0.0178,  0.0197, -0.0026],\n",
       "                       [ 0.0058, -0.0128, -0.0380,  ...,  0.0085, -0.0138,  0.0002],\n",
       "                       [ 0.0118,  0.0035,  0.0288,  ...,  0.0050,  0.0327,  0.0368]],\n",
       "                      device='cuda:0')),\n",
       "              ('encoder.attention.Wa.weight',\n",
       "               tensor([[ 5.3163e-02,  2.2673e-02,  2.2770e-02,  4.8760e-02,  2.1898e-02,\n",
       "                         1.1753e-02,  2.1325e-02,  1.2618e-02,  3.0486e-02,  4.3030e-02,\n",
       "                         1.7309e-02, -6.0415e-02,  2.6244e-02,  3.6316e-02, -2.4296e-02,\n",
       "                         2.2992e-02,  1.6848e-02,  9.2269e-03, -6.1331e-02,  7.7286e-02,\n",
       "                         4.1395e-02, -1.6205e-02,  7.9956e-02,  5.3710e-02,  2.9828e-02,\n",
       "                         2.0612e-02, -2.8746e-02,  4.1507e-02, -3.5445e-02,  4.1845e-02,\n",
       "                        -2.2155e-02, -1.5318e-02, -6.4436e-03,  1.0088e-02, -2.4725e-03,\n",
       "                         3.3815e-02, -2.8891e-02,  3.0522e-03,  2.7591e-02,  2.9854e-02,\n",
       "                         1.9374e-02, -6.8471e-02, -4.2731e-02,  1.0445e-02, -2.3909e-02,\n",
       "                         9.7341e-03,  5.1869e-02,  3.4728e-02,  3.6351e-02,  6.1779e-02,\n",
       "                         6.3179e-03, -1.4084e-02,  3.7032e-02,  4.5400e-02, -2.0895e-02,\n",
       "                         4.8452e-02, -1.0905e-03,  1.3008e-02, -7.2084e-02, -6.2817e-03,\n",
       "                         3.3692e-02, -5.1389e-02, -1.4743e-02,  3.9682e-02, -9.7316e-01,\n",
       "                        -2.7058e-02, -4.7741e-02, -2.8356e-02, -2.6493e-02,  3.5511e-02,\n",
       "                         5.4196e-02, -2.1510e-02, -4.0671e-02, -5.8299e-02,  4.9386e-02,\n",
       "                        -2.2741e-02, -5.6708e-02, -4.8945e-02,  1.4843e-02,  2.8933e-02,\n",
       "                         7.2270e-02,  3.7625e-02, -3.5048e-02,  4.9166e-02,  1.3761e-02,\n",
       "                         5.6536e-02,  4.0149e-03, -6.5144e-03, -5.5472e-02,  4.2615e-02,\n",
       "                        -2.7881e-02, -4.5592e-02, -2.6798e-02,  4.6890e-02,  1.5382e-02,\n",
       "                        -1.0037e-02, -1.9696e-02, -3.2791e-02,  8.8813e-02,  1.3804e-02,\n",
       "                         1.2307e-01,  3.7240e-03, -1.1669e-02, -3.5506e-02,  2.4871e-02,\n",
       "                        -2.5264e-02,  7.3422e-02,  7.2969e-03,  2.8543e-02,  2.7186e-02,\n",
       "                        -4.2352e-02,  4.3675e-02, -4.3027e-02,  4.5218e-02, -3.3278e-03,\n",
       "                         3.8584e-02, -5.0444e-02,  4.0179e-02,  4.9176e-02, -3.0201e-02,\n",
       "                        -1.6179e-02,  3.3596e-02, -2.6630e-02, -7.1991e-03,  4.0612e-02,\n",
       "                         3.4120e-02, -1.7876e-02,  2.5490e-02,  3.4647e-02,  4.9331e-02,\n",
       "                        -4.4580e-02,  3.6850e-02,  4.5095e-02,  3.4929e-02,  1.0111e-02,\n",
       "                        -5.0962e-02, -1.0542e-02,  5.3195e-02, -3.3478e-02,  5.1380e-02,\n",
       "                        -6.0321e-02, -1.7252e-02, -4.9934e-03,  1.5816e-02,  7.3725e-02,\n",
       "                         1.7637e-03, -1.3255e-02, -5.5834e-02, -5.6574e-02,  2.9583e-02,\n",
       "                         2.3585e-02,  6.4308e-02, -3.5434e-02, -4.5031e-02,  5.4796e-02,\n",
       "                        -3.2395e-02,  4.1696e-02, -3.9244e-02, -3.0729e-02,  2.7244e-02,\n",
       "                        -4.1665e-02,  2.2339e-02, -4.5949e-02, -6.3107e-02, -5.1450e-02,\n",
       "                         7.4793e-02,  5.3250e-02, -3.6604e-02,  4.8269e-02, -7.6457e-03,\n",
       "                        -3.1539e-02, -1.1729e-02,  2.3322e-02,  1.0277e-02,  3.9763e-02,\n",
       "                        -3.1997e-02, -6.0703e-02,  5.6999e-02, -3.5188e-02,  5.1581e-02,\n",
       "                         8.1801e-03, -7.2579e-03,  6.2806e-03,  3.0050e-02, -4.0151e-04,\n",
       "                        -8.2152e-02, -1.5859e-02, -2.2487e-02,  5.7853e-02,  2.1833e-02,\n",
       "                        -4.6024e-02,  1.8269e-02,  3.3751e-03, -5.7466e-02,  6.9124e-02,\n",
       "                        -3.3032e-02,  1.8468e-02, -1.7662e-02, -5.9974e-02,  2.9993e-02]],\n",
       "                      device='cuda:0')),\n",
       "              ('fc_in.weight',\n",
       "               tensor([[-0.0026, -0.0089,  0.0043,  ..., -0.0042,  0.0044, -0.0077],\n",
       "                       [ 0.0173,  0.0221, -0.0208,  ...,  0.0226,  0.0105, -0.0115],\n",
       "                       [-0.0101,  0.0022, -0.0189,  ...,  0.0175, -0.0068,  0.0229],\n",
       "                       ...,\n",
       "                       [-0.0080, -0.0047,  0.0340,  ..., -0.0116, -0.0125, -0.0177],\n",
       "                       [ 0.0050,  0.0170, -0.0298,  ..., -0.0136, -0.0109,  0.0219],\n",
       "                       [-0.0002, -0.0107, -0.0065,  ...,  0.0060,  0.0107, -0.0042]],\n",
       "                      device='cuda:0')),\n",
       "              ('fc_in.bias',\n",
       "               tensor([ 0.0110,  0.1429,  0.0177, -0.0074,  0.0871,  0.1669,  0.2714,  0.0238,\n",
       "                        0.1230, -0.1408, -0.0887,  0.2339, -0.0745,  0.1592,  0.0177, -0.2251,\n",
       "                        0.0515,  0.0140,  0.0237, -0.0024, -0.1297,  0.0537, -0.2412, -0.1633,\n",
       "                       -0.2300,  0.0989,  0.0309,  0.0776, -0.3000,  0.0971,  0.0841, -0.1014,\n",
       "                       -0.0591, -0.1501,  0.0015, -0.1329, -0.0445, -0.3351, -0.0828,  0.1393,\n",
       "                       -0.2531,  0.0578, -0.2429, -0.0024,  0.0313,  0.1346, -0.0031, -0.1442,\n",
       "                        0.1512,  0.2008,  0.1607,  0.0057, -0.0873,  0.2494, -0.1686,  0.2534,\n",
       "                       -0.0193,  0.0301, -0.1675,  0.0103, -0.1079, -0.0153, -0.2670, -0.2189,\n",
       "                       -0.0818, -0.2273, -0.0310, -0.4140, -0.1403, -0.0977,  0.1365, -0.0969,\n",
       "                        0.0990, -0.1133,  0.0655, -0.0782,  0.0639,  0.0722,  0.0412,  0.0567,\n",
       "                       -0.0702, -0.2733,  0.0813, -0.2261,  0.2373,  0.0399,  0.0496, -0.0589,\n",
       "                       -0.3270,  0.0262,  0.0875, -0.1107,  0.1200, -0.0941, -0.1083, -0.0459,\n",
       "                        0.0631,  0.2874, -0.1980, -0.2443, -0.2094, -0.0187, -0.1472, -0.2774,\n",
       "                       -0.1146,  0.1154,  0.1130,  0.2240, -0.0335, -0.0596,  0.1517, -0.0967,\n",
       "                        0.1570,  0.0401,  0.1510, -0.1010,  0.3483,  0.3021, -0.1608,  0.1362,\n",
       "                        0.1027, -0.1743, -0.0275,  0.0124, -0.1537, -0.0189,  0.1989, -0.1103,\n",
       "                       -0.0019,  0.1022,  0.0589,  0.1832,  0.0245,  0.1126, -0.2093,  0.0906,\n",
       "                        0.0829, -0.0887, -0.0734, -0.0389,  0.0524,  0.0155,  0.1010,  0.0126,\n",
       "                       -0.0405, -0.2263,  0.0584,  0.0773, -0.0797,  0.0567, -0.2713, -0.1444,\n",
       "                       -0.2298,  0.0906,  0.1196, -0.0858,  0.0830,  0.1184,  0.3134,  0.1652,\n",
       "                       -0.0856, -0.1187, -0.2539, -0.1872,  0.0492,  0.4748,  0.1168,  0.0168,\n",
       "                        0.1677,  0.0648, -0.2182,  0.1080, -0.0079, -0.1296,  0.0108,  0.0282,\n",
       "                       -0.1207,  0.1273,  0.1401, -0.0022, -0.0021,  0.0324,  0.0955, -0.1402,\n",
       "                       -0.2424,  0.1368, -0.0082,  0.1871,  0.0868,  0.1294,  0.0069, -0.1590,\n",
       "                       -0.0364, -0.2435, -0.1593,  0.1255, -0.0031,  0.1178,  0.2567, -0.1173,\n",
       "                       -0.0179, -0.1625, -0.1541,  0.1172, -0.1269,  0.2603,  0.2635, -0.0303,\n",
       "                       -0.1034,  0.1355, -0.4074,  0.1905, -0.1749, -0.1931, -0.1488,  0.1297,\n",
       "                        0.1264,  0.2604, -0.1529, -0.0260,  0.1642, -0.1657, -0.2431,  0.1690,\n",
       "                        0.2510,  0.0029,  0.2862,  0.0494,  0.2126,  0.1695,  0.1816, -0.0179,\n",
       "                        0.1887,  0.2198, -0.0199, -0.1634,  0.1805, -0.2830,  0.0363,  0.0054,\n",
       "                       -0.0418, -0.2664,  0.1040, -0.2287, -0.0895,  0.0120, -0.1845, -0.3082,\n",
       "                       -0.2041, -0.0939,  0.0711,  0.0533, -0.1639,  0.2440, -0.1389,  0.2368,\n",
       "                       -0.0386,  0.1035,  0.1559, -0.1892, -0.1612,  0.2120,  0.2647, -0.1280,\n",
       "                        0.0321, -0.0196,  0.0560, -0.1766, -0.0113, -0.1899, -0.0531, -0.1607,\n",
       "                        0.0750,  0.1576,  0.1231, -0.1984, -0.0916,  0.1011,  0.0721,  0.1125,\n",
       "                        0.1971, -0.3044, -0.0532,  0.0914,  0.0008,  0.1348, -0.0523, -0.1974,\n",
       "                        0.2281, -0.1702, -0.1401,  0.0771,  0.2743,  0.0714,  0.1626,  0.2699,\n",
       "                       -0.2384, -0.0716, -0.2115, -0.0829, -0.3387,  0.0698, -0.1886,  0.2712,\n",
       "                       -0.2360,  0.1511,  0.1747,  0.0076, -0.1209,  0.0674, -0.0536, -0.3309,\n",
       "                        0.2552, -0.1984, -0.1888, -0.1671,  0.3069,  0.2500,  0.0612,  0.3650,\n",
       "                       -0.0015,  0.1060,  0.1277, -0.0163,  0.2201, -0.2201, -0.2726,  0.1131,\n",
       "                        0.0406,  0.1618, -0.1214,  0.1326, -0.0427, -0.0327, -0.0056, -0.1651,\n",
       "                        0.0999,  0.3196, -0.0615,  0.0587,  0.1422,  0.0723, -0.1313, -0.3685,\n",
       "                        0.2537, -0.3243, -0.0242,  0.1424, -0.3272, -0.2401, -0.0268,  0.1519,\n",
       "                        0.0235, -0.0455,  0.1373,  0.0888,  0.0053,  0.2147,  0.0351, -0.1092,\n",
       "                       -0.0212,  0.0426,  0.1328, -0.2813, -0.1377,  0.0660,  0.0143, -0.2449,\n",
       "                        0.0501, -0.0098, -0.3650, -0.1992, -0.1968,  0.0040,  0.1265,  0.1666,\n",
       "                       -0.0205,  0.1895,  0.3447, -0.1829,  0.2911,  0.3252, -0.1546,  0.0855,\n",
       "                       -0.1091,  0.0400,  0.1938, -0.0691,  0.0474,  0.0470,  0.0382,  0.0950,\n",
       "                       -0.3476, -0.0048, -0.1623,  0.1502,  0.0325, -0.0804, -0.0661,  0.1480],\n",
       "                      device='cuda:0')),\n",
       "              ('fcs.0.weight',\n",
       "               tensor([[ 0.0131,  0.0057, -0.0368,  ..., -0.0252,  0.0608, -0.0053],\n",
       "                       [ 0.0633,  0.0025, -0.0523,  ...,  0.0175,  0.0269,  0.0140],\n",
       "                       [ 0.0676, -0.0078, -0.0170,  ...,  0.0421,  0.0263, -0.1144],\n",
       "                       ...,\n",
       "                       [ 0.0330,  0.0198, -0.0076,  ...,  0.0180, -0.0087,  0.0032],\n",
       "                       [ 0.0266,  0.0649, -0.0528,  ...,  0.0434, -0.0043, -0.0127],\n",
       "                       [ 0.0163, -0.0337, -0.0267,  ..., -0.0044, -0.0471,  0.0468]],\n",
       "                      device='cuda:0')),\n",
       "              ('fcs.0.bias',\n",
       "               tensor([ 5.2436e-02, -1.8963e-01, -3.7874e-01, -7.1364e-02, -1.5950e-01,\n",
       "                       -1.3836e-01, -1.2494e-01, -7.5724e-02, -1.3021e-01, -2.8392e-01,\n",
       "                       -6.6271e-02, -1.3158e-01, -6.1336e-02, -1.4258e-01, -3.9951e-02,\n",
       "                       -2.4744e-01, -5.6227e-01, -1.3756e-01, -2.4229e-01, -2.2639e-01,\n",
       "                       -1.5629e-01, -9.7698e-02, -2.1722e-01, -4.4048e-01,  9.8946e-03,\n",
       "                       -1.5088e-01, -7.1516e-02, -1.3282e-01,  4.6399e-02, -2.9151e-01,\n",
       "                       -1.1660e-01, -1.6335e-01, -2.0824e-01, -2.8097e-01,  1.4780e-02,\n",
       "                       -3.0663e-01, -9.1789e-02,  4.3680e-02, -2.4396e-01, -4.4702e-02,\n",
       "                       -1.1343e-01, -3.4921e-01, -1.6549e-01, -2.1182e-01, -1.2444e-01,\n",
       "                       -2.0794e-01, -1.4073e-01, -1.2063e-01, -5.3357e-01, -6.4465e-01,\n",
       "                       -2.1435e-02,  1.6585e-02, -1.8263e-01, -1.5625e-01, -6.1891e-02,\n",
       "                       -1.1114e-01, -1.2299e-01, -2.6518e-03, -1.7311e-01, -1.2939e-01,\n",
       "                       -2.0740e-01, -9.0828e-02, -2.4834e-01, -1.2591e-01, -2.0122e-01,\n",
       "                       -3.9188e-01, -2.0548e-01, -1.9180e-01, -7.0202e-02, -1.7257e-01,\n",
       "                       -6.7499e-02, -1.2750e-01, -1.5897e-01, -1.1633e-01, -8.4581e-02,\n",
       "                       -2.7276e-01, -1.8128e-04, -9.2757e-02, -6.4648e-02, -9.1871e-02,\n",
       "                       -2.3276e-01, -1.7498e-01, -9.4442e-02, -9.8477e-02, -2.2385e-02,\n",
       "                       -1.1477e-01, -5.2290e-02, -1.1922e-01,  5.9448e-02, -1.8184e-01,\n",
       "                       -3.1728e-01, -1.4030e-01, -1.6866e-01,  4.1634e-02, -8.1361e-03,\n",
       "                       -1.9765e-01, -5.9328e-02, -6.1523e-01, -4.3527e-01, -6.8627e-02,\n",
       "                       -2.1683e-01, -3.0424e-01, -3.2250e-01, -2.5883e-01, -3.4002e-01,\n",
       "                       -2.8978e-01, -1.7828e-01, -6.0509e-02, -3.2335e-01, -1.1708e-01,\n",
       "                       -4.2618e-01, -1.2527e-01, -2.4878e-02, -2.0131e-01,  5.2152e-02,\n",
       "                        3.3556e-02, -6.3187e-02,  1.9747e-03, -2.1510e-01, -7.7875e-02,\n",
       "                       -2.3730e-01, -1.0726e-01, -9.6959e-02, -1.5012e-01, -4.1192e-01,\n",
       "                       -9.0395e-02, -1.2128e-01, -1.8083e-01, -2.1841e-02, -1.2843e-01,\n",
       "                       -4.0463e-02, -1.6509e-01, -2.3115e-01, -6.7743e-02,  2.8734e-02,\n",
       "                       -3.0496e-01, -1.1232e-01, -9.7485e-02, -1.2570e-01, -1.7984e-01,\n",
       "                       -1.9376e-01, -3.8424e-01, -3.0335e-01, -1.9093e-01, -2.7644e-01,\n",
       "                       -2.5787e-01, -4.2082e-02,  1.3726e-01, -2.5773e-01,  2.4732e-02,\n",
       "                       -2.0846e-01, -1.8897e-01, -3.4616e-01, -1.2641e-01, -3.0738e-01,\n",
       "                       -3.3506e-02, -3.6640e-02, -3.2498e-01, -3.3202e-01, -1.6238e-01,\n",
       "                       -1.6802e-01, -1.4861e-01, -3.1219e-02, -4.9802e-01, -1.2169e-01,\n",
       "                       -2.1839e-01, -1.3072e-01, -3.6547e-02, -1.4315e-01, -1.9174e-01,\n",
       "                       -2.3132e-01, -4.7549e-02, -7.9336e-02, -4.6839e-01, -1.5567e-01,\n",
       "                       -1.9548e-01, -1.9158e-01, -2.7567e-02, -1.1106e-01, -2.6410e-02,\n",
       "                       -2.0951e-02, -2.6139e-01, -1.0248e-01, -4.6001e-02, -6.6719e-03,\n",
       "                       -4.5791e-02, -2.3884e-01, -4.5774e-02, -2.5590e-01, -3.3271e-01,\n",
       "                       -5.1218e-02, -1.7306e-01, -1.0839e-01, -5.8363e-02, -1.5394e-01,\n",
       "                       -1.0369e-01, -8.5334e-02, -1.2032e-01, -9.7377e-02,  1.3572e-02,\n",
       "                       -2.5762e-01, -5.7483e-02, -8.9581e-02, -2.3912e-01, -3.2087e-01,\n",
       "                       -4.6235e-02, -4.4176e-02, -3.5627e-01, -1.7163e-01, -1.1232e-01,\n",
       "                       -1.7130e-02, -1.1040e-01, -1.1608e-01, -6.7472e-02, -3.9411e-01,\n",
       "                       -1.5620e-01, -1.7692e-01, -1.3426e-01, -1.2082e-01, -3.4306e-01,\n",
       "                       -2.9804e-01, -1.3202e-01, -1.2831e-01, -1.7948e-01, -1.7424e-01,\n",
       "                       -6.0741e-01,  6.8321e-03, -7.1359e-01, -1.1904e-01, -1.8838e-03,\n",
       "                       -2.0820e-01, -1.1490e-01, -1.1118e-01, -1.1809e-01, -1.6508e-01,\n",
       "                       -1.4523e-01, -2.9148e-02,  3.1383e-02, -1.6892e-01, -2.0946e-01,\n",
       "                       -8.9506e-02, -9.7357e-02, -3.9461e-03, -2.5255e-01, -5.3341e-03,\n",
       "                       -2.0830e-01, -8.1575e-02, -5.9908e-03, -1.4132e-01, -1.7558e-01,\n",
       "                       -2.3952e-01, -1.1982e-01, -2.3470e-01, -1.0786e-01, -8.2759e-01,\n",
       "                       -2.6268e-01, -4.6836e-03, -1.1815e-01, -1.0562e-01, -2.4002e-01,\n",
       "                       -1.5731e-02, -6.5275e-02, -2.6896e-01, -1.0285e-01, -2.1629e-01,\n",
       "                       -2.4669e-01, -2.9148e-02, -4.1668e-02, -2.9006e-02, -3.1227e-01,\n",
       "                       -2.3249e-01, -1.0837e-01, -2.3623e-01, -1.6716e-01, -1.4624e-02,\n",
       "                       -4.8331e-01, -1.4263e-01, -9.3223e-02, -1.1413e-02,  2.2495e-02,\n",
       "                       -2.3584e-01, -2.0701e-01, -1.3246e-01, -2.3565e-01, -2.1785e-01,\n",
       "                       -1.0612e-01, -1.5115e-01, -1.1397e-01, -2.0139e-01, -1.2530e-02,\n",
       "                       -1.6913e-01, -7.0883e-02, -1.7757e-01, -2.3315e-01,  5.7864e-02,\n",
       "                       -2.4575e-01, -7.9886e-01, -5.3805e-02, -6.9136e-02, -6.3299e-02,\n",
       "                       -2.4263e-01, -4.0695e-01, -5.9878e-02, -1.8291e-01, -1.5031e-01,\n",
       "                       -2.2611e-02, -1.4567e-01, -8.1621e-02, -4.2203e-01, -1.4385e-01,\n",
       "                        1.3260e-01, -1.4319e-01, -6.4841e-02, -1.4168e-01, -3.3069e-01,\n",
       "                       -1.4059e-02, -2.9052e-01, -4.7036e-02, -1.7964e-01,  3.0837e-02,\n",
       "                       -3.9217e-01, -2.2838e-01, -2.0265e-01,  6.5471e-02, -9.6144e-02,\n",
       "                       -2.6776e-01, -2.8013e-02, -4.3649e-03, -8.7205e-02, -1.5434e-01,\n",
       "                       -5.8235e-02, -1.0475e-01, -5.9546e-01, -1.2350e-01, -1.6935e-01,\n",
       "                       -3.6915e-01, -8.3351e-01, -1.3229e-01,  2.5192e-02, -1.3104e-01,\n",
       "                       -1.1744e-01, -2.0916e-01, -2.4101e-02, -1.3346e-01, -1.7736e-01,\n",
       "                       -1.8028e-01, -2.4609e-02, -1.7860e-02, -9.6242e-02, -1.8921e-01,\n",
       "                       -1.4295e-01, -3.1605e-02, -4.4716e-02, -7.9498e-03, -1.9028e-01,\n",
       "                       -4.7022e-01, -2.3481e-02, -1.9567e-01, -8.1462e-02, -1.7115e-01,\n",
       "                       -2.8221e-01, -1.4878e-01, -2.1853e-01, -2.1664e-01, -1.5033e-01,\n",
       "                       -9.5745e-02, -1.0393e-01, -2.4044e-03, -2.0636e-01, -1.4219e-01,\n",
       "                       -1.0950e-01, -2.1401e-01, -2.4413e-01, -2.2807e-01, -2.4360e-01,\n",
       "                       -2.1175e-01,  3.6965e-02,  6.1425e-02, -1.4663e-01, -1.1319e-01,\n",
       "                       -1.5476e-01, -1.2729e-01, -1.8595e-01, -2.6007e-01, -3.5290e-01,\n",
       "                       -9.5295e-02, -7.8947e-02, -9.3785e-02, -9.8779e-02, -1.7406e-01,\n",
       "                       -2.2632e-01, -1.6590e-01, -4.8847e-01,  6.5673e-02, -8.6501e-02,\n",
       "                       -1.7854e-01,  1.6379e-02, -9.3477e-02,  5.8878e-03,  1.5037e-01],\n",
       "                      device='cuda:0')),\n",
       "              ('fc_out.weight',\n",
       "               tensor([[ 0.1339, -0.0223, -0.0681,  ...,  0.0227,  0.0902,  0.0424],\n",
       "                       [-0.0866,  0.0348,  0.0886,  ..., -0.0473, -0.1283,  0.0260],\n",
       "                       [-0.0257, -0.0870, -0.0561,  ..., -0.0694, -0.0572, -0.1194]],\n",
       "                      device='cuda:0')),\n",
       "              ('fc_out.bias',\n",
       "               tensor([-0.0977, -0.2932,  0.4495], device='cuda:0'))]),\n",
       " 'options': {'attention_layer_param': 200,\n",
       "  'char_embedding_dim': 100,\n",
       "  'dataset': 'snli',\n",
       "  'device': 'cuda',\n",
       "  'dropout': 0.3,\n",
       "  'embedding_dim': 300,\n",
       "  'fcs': 1,\n",
       "  'hidden_size': 400,\n",
       "  'max_word_len': 10,\n",
       "  'num_layers': 1,\n",
       "  'padding_idx': 1,\n",
       "  'results_dir': 'results',\n",
       "  'use_char_emb': False,\n",
       "  'use_glove': True,\n",
       "  'vocab_size': 33934}}"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}