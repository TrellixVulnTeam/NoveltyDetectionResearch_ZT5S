{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.defaults import *\n",
    "from src.datasets.novelty import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_nli = 'NLI-87'\n",
    "\n",
    "# download_models_from_neptune(\"NLI-81\")\n",
    "field = load_field(load_nli)\n",
    "# field = None\n",
    "\n",
    "\n",
    "dataset_conf = {'dataset': 'dlnd', 'max_num_sent': 60,\"sent_tokenizer\":\"spacy\",\"batch_size\":4,\"device\":\"cuda\"}\n",
    "# dataset_conf = {'dataset': 'dlnd', 'max_num_sent': 50,\"sent_tokenizer\":\"spacy\", \"tokenizer\":'spacy',\"max_len\":50,\"batch_size\":32,\"device\":\"cuda\"}\n",
    "model_conf = {'results_dir': 'results', 'device': 'cuda', 'dropout': 0.3, 'dataset': 'dlnd', 'hidden_size': 400, 'use_glove': False,\"num_layers\":2,\"dense_net_growth_rate\":20,\"dense_net_layers\":3,\"dense_net_transition_rate\":0.2,\"dense_net_kernel_size\":3,\n",
    "\"dense_net_channels\":100,\"dense_net_first_scale_down_ratio\":0.3,\n",
    "\"first_scale_down_kernel\":1,\n",
    "\"max_num_sent\":60,\n",
    "\"dropout\":[0.3,0.3,0.3,0.3,0.3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dlnd(dataset_conf,sentence_field = field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n[torchtext.data.batch.Batch of size 4]\n\t[.source]:[torch.cuda.LongTensor of size 4x60x50 (GPU 0)]\n\t[.target]:[torch.cuda.LongTensor of size 4x60x50 (GPU 0)]\n\t[.label]:[torch.cuda.LongTensor of size 4 (GPU 0)]\n"
     ]
    }
   ],
   "source": [
    "for i in data.train_iter:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.nli_models import *\n",
    "from src.model.novelty_models import *\n",
    "\n",
    "def load_encoder(enc_data):\n",
    "    if enc_data['options'].get(\"attention_layer_param\",0)==0:\n",
    "        model = bilstm_snli(enc_data[\"options\"])\n",
    "    elif enc_data['options'].get(\"r\",0)==0:\n",
    "        model = attn_bilstm_snli(enc_data[\"options\"])\n",
    "    else:\n",
    "        model = struc_attn_snli(enc_data[\"options\"])\n",
    "    return model\n",
    "\n",
    "nli_model_data = load_encoder_data(load_nli)\n",
    "nli_model_data['options'][\"use_glove\"] = False\n",
    "encoder = load_encoder(nli_model_data).encoder\n",
    "model_conf[\"encoder_dim\"] = nli_model_data[\"options\"][\"hidden_size\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, size, num_layers, dropout):\n",
    "\n",
    "        super(Highway, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.nonlinear = nn.ModuleList(\n",
    "            [nn.Linear(size, size) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "\n",
    "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.f = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tensor with shape of [batch_size, size]\n",
    "        :return: tensor with shape of [batch_size, size]\n",
    "        applies σ(x) ⨀ (f(G(x))) + (1 - σ(x)) ⨀ (Q(x)) transformation | G and Q is affine transformation,\n",
    "        f is non-linear transformation, σ(x) is affine transformation with sigmoid non-linearition\n",
    "        and ⨀ is element-wise multiplication\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            gate = torch.sigmoid(self.gate[layer](x))\n",
    "\n",
    "            nonlinear = self.f(self.nonlinear[layer](x))\n",
    "            linear = self.linear[layer](x)\n",
    "            linear = self.dropout(linear)\n",
    "            x = gate * nonlinear + (1 - gate) * linear\n",
    "        return x\n",
    "\n",
    "\n",
    "class self_attention(nn.Module):\n",
    "    \"\"\"[summary]\n",
    "    Self attention modeled as demonstrated in NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE paper\n",
    "\n",
    "    REF: https://github.com/YerevaNN/DIIN-in-Keras/blob/master/layers/encoding.py\n",
    "\n",
    "    # P = P_hw\n",
    "    # itr_attn = P_itrAtt\n",
    "    # encoding = P_enc\n",
    "    # The paper takes inputs to be P(_hw) as an example and then computes the same thing for H,\n",
    "    # therefore we'll name our inputs P too.\n",
    "\n",
    "    # Input of encoding is P with shape (batch, p, d). It would be (batch, h, d) for hypothesis\n",
    "    # Construct alphaP of shape (batch, p, 3*d, p)\n",
    "    # A = dot(w_itr_att, alphaP)\n",
    "\n",
    "    # alphaP consists of 3*d rows along 2nd axis\n",
    "    # 1. up   -> first  d items represent P[i]\n",
    "    # 2. mid  -> second d items represent P[j]\n",
    "    # 3. down -> final items represent alpha(P[i], P[j]) which is element-wise product of P[i] and P[j] = P[i]*P[j]\n",
    "\n",
    "    # If we look at one slice of alphaP we'll see that it has the following elements:\n",
    "    # ----------------------------------------\n",
    "    # P[i][0], P[i][0], P[i][0], ... P[i][0]   ▲\n",
    "    # P[i][1], P[i][1], P[i][1], ... P[i][1]   |\n",
    "    # P[i][2], P[i][2], P[i][2], ... P[i][2]   |\n",
    "    # ...                              ...     | up\n",
    "    #      ...                         ...     |\n",
    "    #             ...                  ...     |\n",
    "    # P[i][d], P[i][d], P[i][d], ... P[i][d]   ▼\n",
    "    # ----------------------------------------\n",
    "    # P[0][0], P[1][0], P[2][0], ... P[p][0]   ▲\n",
    "    # P[0][1], P[1][1], P[2][1], ... P[p][1]   |\n",
    "    # P[0][2], P[1][2], P[2][2], ... P[p][2]   |\n",
    "    # ...                              ...     | mid\n",
    "    #      ...                         ...     |\n",
    "    #             ...                  ...     |\n",
    "    # P[0][d], P[1][d], P[2][d], ... P[p][d]   ▼\n",
    "    # ----------------------------------------\n",
    "    #                                          ▲\n",
    "    #                                          |\n",
    "    #                                          |\n",
    "    #               up * mid                   | down\n",
    "    #          element-wise product            |\n",
    "    #                                          |\n",
    "    #                                          ▼\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # For every slice(i) the up part changes its P[i] values\n",
    "    # The middle part is repeated p times in depth (for every i)\n",
    "    # So we can get the middle part by doing the following:\n",
    "    # mid = broadcast(P) -> to get tensor of shape (batch, p, d, p)\n",
    "    # As we can notice up is the same mid, but with changed axis, so to obtain up from mid we can do:\n",
    "    # up = swap_axes(mid, axis1=0, axis2=2)\n",
    "\n",
    "    # P_itr_attn[i] = sum of for j = 1...p:\n",
    "    #                           s = sum(for k = 1...p:  e^A[k][j]\n",
    "    #                           ( e^A[i][j] / s ) * P[j]  --> P[j] is the j-th row, while the first part is a number\n",
    "    # So P_itr_attn is the weighted sum of P\n",
    "    # SA is column-wise soft-max applied on A\n",
    "    # P_itr_attn[i] is the sum of all rows of P scaled by i-th row of SA\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conf):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(3 * conf[\"hidden_size\"], 1, bias=False)\n",
    "        self.dropout = nn.Dropout(conf[\"dropout\"][2])\n",
    "\n",
    "    def forward(self, p):\n",
    "        # p = [B,P,D]\n",
    "        p_dim = p.shape[1]\n",
    "        mid = p.unsqueeze(3).expand(-1, -1, -1, p_dim)\n",
    "        # min = [B,P,D,P]\n",
    "        up = mid.permute(0, 3, 2, 1)\n",
    "        alpha = torch.cat([up, mid, up * mid], dim=2)\n",
    "        A = (self.w.weight @ alpha).squeeze(2)\n",
    "        A = self.dropout(A)\n",
    "        sA = A.softmax(dim=2)\n",
    "        itr_attn = torch.bmm(sA, p)\n",
    "        return itr_attn\n",
    "\n",
    "\n",
    "class fuse_gate(nn.Module):\n",
    "    \"\"\"[summary]\n",
    "    Fuse gate is used to provide a Skip connection for the encoding and the attended output.\n",
    "    The author uses:\n",
    "    zi = tanh(W1 * [P:P_att]+b1)\n",
    "    ri = Sigmoid(W2 * [P:P_att]+b2)\n",
    "    fi = Sigmoid(W3 * [P:P_att]+b3)\n",
    "    P_new  = r dot P + fi dot zi\n",
    "\n",
    "    W1,W2,W3 = Linear(2d,d)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conf):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(conf[\"hidden_size\"] * 2, conf[\"hidden_size\"])\n",
    "        self.fc2 = nn.Linear(conf[\"hidden_size\"] * 2, conf[\"hidden_size\"])\n",
    "        self.fc3 = nn.Linear(conf[\"hidden_size\"] * 2, conf[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(conf[\"dropout\"][3])\n",
    "\n",
    "    def forward(self, p_hat_i, p_dash_i):\n",
    "        x = torch.cat([p_hat_i, p_dash_i], dim=2)\n",
    "        z = torch.tanh(self.dropout(self.fc1(x)))\n",
    "        r = torch.sigmoid(self.dropout(self.fc1(x)))\n",
    "        f = torch.sigmoid(self.dropout(self.fc1(x)))\n",
    "        enc = r * p_hat_i + f * z\n",
    "        return enc\n",
    "\n",
    "\n",
    "class interaction(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, p, h):\n",
    "        p = p.unsqueeze(2)\n",
    "        h = h.unsqueeze(1)\n",
    "        return p * h\n",
    "\n",
    "\n",
    "class Dense_net_block(nn.Module):\n",
    "    def __init__(self, outChannels, growth_rate, kernel_size):\n",
    "        super(Dense_net_block, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            outChannels, growth_rate, kernel_size=kernel_size, bias=False, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ft = F.relu(self.conv(x))\n",
    "        out = torch.cat((x, ft), dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Dense_net_transition(nn.Module):\n",
    "    def __init__(self, nChannels, outChannels):\n",
    "        super(Dense_net_transition, self).__init__()\n",
    "        self.conv = nn.Conv2d(nChannels, outChannels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = F.max_pool2d(out, (2, 2), (2, 2), padding=0)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate, reduction, nDenseBlocks, kernel_size):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, kernel_size)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        nOutChannels = int(math.floor(nChannels * reduction))\n",
    "        self.trans1 = Dense_net_transition(nChannels, nOutChannels)\n",
    "        nChannels = nOutChannels\n",
    "\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, kernel_size)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        nOutChannels = int(math.floor(nChannels * reduction))\n",
    "        self.trans2 = Dense_net_transition(nChannels, nOutChannels)\n",
    "        nChannels = nOutChannels\n",
    "\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, kernel_size)\n",
    "        nChannels += nDenseBlocks * growthRate\n",
    "        nOutChannels = int(math.floor(nChannels * reduction))\n",
    "        self.trans3 = Dense_net_transition(nChannels, nOutChannels)\n",
    "\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, kernel_size):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            layers.append(Dense_net_block(nChannels, growthRate, kernel_size))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        out = self.trans1(self.dense1(x))\n",
    "        # print(out.shape)\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        # print(out.shape)\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DIIN(nn.Module):\n",
    "    def __init__(self, conf, encoder):\n",
    "        super().__init__()\n",
    "        self.conf = conf\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.num_sent = conf[\"max_num_sent\"]\n",
    "\n",
    "        self.template = nn.Parameter(torch.zeros((1)), requires_grad=True)\n",
    "        self.dropout = nn.Dropout(conf[\"dropout\"][4])\n",
    "\n",
    "        self.translate = nn.Linear(2*conf[\"hidden_size\"], self.conf['hidden_size'])\n",
    "        self.highway = Highway(self.conf[\"hidden_size\"], conf[\"num_layers\"],conf[\"dropout\"][1])\n",
    "        self.attn = self_attention(self.conf)\n",
    "        self.fuse = fuse_gate(self.conf)\n",
    "        self.interact = interaction(self.conf)\n",
    "        self.interaction_cnn = nn.Conv2d(\n",
    "            self.conf[\"hidden_size\"],\n",
    "            int(self.conf[\"hidden_size\"] * self.conf[\"dense_net_first_scale_down_ratio\"]),\n",
    "            self.conf[\"first_scale_down_kernel\"],\n",
    "            padding=0,\n",
    "        )\n",
    "        nChannels = int(self.conf[\"hidden_size\"] * self.conf[\"dense_net_first_scale_down_ratio\"])\n",
    "        \n",
    "        features = self.num_sent\n",
    "\n",
    "        for i in range(3):\n",
    "            nChannels += self.conf[\"dense_net_layers\"] * self.conf[\"dense_net_growth_rate\"]\n",
    "            nOutChannels = int(math.floor(nChannels * self.conf[\"dense_net_transition_rate\"]))\n",
    "            nChannels = nOutChannels\n",
    "            features = features//2\n",
    "        final_layer_size = ((features**2)*nChannels)\n",
    "\n",
    "        self.dense_net = DenseNet(\n",
    "            int(self.conf[\"hidden_size\"] * self.conf[\"dense_net_first_scale_down_ratio\"]),\n",
    "            self.conf[\"dense_net_growth_rate\"],\n",
    "            self.conf[\"dense_net_transition_rate\"],\n",
    "            self.conf[\"dense_net_layers\"], \n",
    "            self.conf[\"dense_net_kernel_size\"],\n",
    "        )\n",
    "        self.fc1 = nn.Linear(final_layer_size, 2)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def encoder_sent(self,inp):\n",
    "        batch_size, num_sent, max_len = inp.shape\n",
    "        x = inp.view(-1, max_len)\n",
    "\n",
    "        x_padded_idx = x.sum(dim=1) != 0\n",
    "        x_enc = []\n",
    "        for sub_batch in x[x_padded_idx].split(64):\n",
    "            x_enc.append(self.encoder(sub_batch, None))\n",
    "        x_enc = torch.cat(x_enc, dim=0)\n",
    "\n",
    "        x_enc_t = torch.zeros((batch_size * num_sent, x_enc.size(1))).to(\n",
    "            self.template.device\n",
    "        )\n",
    "\n",
    "        x_enc_t[x_padded_idx] = x_enc\n",
    "        x_enc_t = x_enc_t.view(batch_size, num_sent, -1)\n",
    "\n",
    "        embedded = self.dropout(self.translate(x_enc_t))\n",
    "        embedded = self.act(embedded)\n",
    "        return embedded\n",
    "\n",
    "    def encode_attn(self, x):\n",
    "        x = self.encoder_sent(x)\n",
    "        x = self.highway(x)\n",
    "        x_att = self.attn(x)\n",
    "        enc = self.fuse(x, x_att)\n",
    "        return enc\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        batch_size = source.shape[0]\n",
    "        s_enc = self.encode_attn(source)\n",
    "        # print(p_enc.shape)\n",
    "        t_enc = self.encode_attn(target)\n",
    "        intr = self.interact(s_enc, t_enc).permute(0, 3, 1, 2)\n",
    "        # print(intr.shape)\n",
    "        fm = self.interaction_cnn(intr)\n",
    "        # print(fm.shape)\n",
    "        dense = self.dense_net(fm)\n",
    "        # print(dense.shape)\n",
    "        opt = self.fc1(dense.reshape(batch_size, -1))\n",
    "        # print(opt.shape)\n",
    "        return opt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DIIN(model_conf,encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0355, 0.0098],\n",
       "        [0.0356, 0.0098],\n",
       "        [0.0356, 0.0099],\n",
       "        [0.0356, 0.0098]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model(i.source.cpu(),i.target.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Attn_Encoder(\n",
       "  (embedding): Embedding(33934, 300, padding_idx=1)\n",
       "  (translate): Linear(in_features=300, out_features=400, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (lstm_layer): LSTM(400, 400, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (attention): Attention(\n",
       "    (Ws): Linear(in_features=800, out_features=200, bias=False)\n",
       "    (Wa): Linear(in_features=200, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 60, 400])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_layer_param': 200,\n",
       " 'char_embedding_dim': 100,\n",
       " 'dataset': 'snli',\n",
       " 'device': 'cuda',\n",
       " 'dropout': 0.3,\n",
       " 'embedding_dim': 300,\n",
       " 'fcs': 1,\n",
       " 'hidden_size': 400,\n",
       " 'max_word_len': 10,\n",
       " 'num_layers': 1,\n",
       " 'padding_idx': 1,\n",
       " 'results_dir': 'results',\n",
       " 'use_char_emb': False,\n",
       " 'use_glove': False,\n",
       " 'vocab_size': 33934}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "model.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "import numpy as np\n",
    "from torchtext.data import Dataset\n",
    "\n",
    "\n",
    "def iter_folds(data):\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    train_exs_arr = np.array(data.data.examples)\n",
    "    labels = np.array([i.label for i in data.data.examples])\n",
    "    fields = data.data.fields\n",
    "    for train_idx, val_idx in kf.split(train_exs_arr,y = labels):\n",
    "        yield (\n",
    "            Dataset(train_exs_arr[train_idx], fields),\n",
    "            Dataset(train_exs_arr[val_idx], fields),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(<torchtext.data.dataset.Dataset object at 0x7fb59a014ef0>, <torchtext.data.dataset.Dataset object at 0x7fb5991c9ba8>)\n(<torchtext.data.dataset.Dataset object at 0x7fb59a031898>, <torchtext.data.dataset.Dataset object at 0x7fb5991c93c8>)\n(<torchtext.data.dataset.Dataset object at 0x7fb59a014ef0>, <torchtext.data.dataset.Dataset object at 0x7fb5991c9ba8>)\n(<torchtext.data.dataset.Dataset object at 0x7fb59a031898>, <torchtext.data.dataset.Dataset object at 0x7fb5991c93c8>)\n(<torchtext.data.dataset.Dataset object at 0x7fb59a014ef0>, <torchtext.data.dataset.Dataset object at 0x7fb5991c9ba8>)\n(<torchtext.data.dataset.Dataset object at 0x7fb59a031898>, <torchtext.data.dataset.Dataset object at 0x7fb5991c93c8>)\n(<torchtext.data.dataset.Dataset object at 0x7fb59a014ef0>, <torchtext.data.dataset.Dataset object at 0x7fb5991c9ba8>)\n(<torchtext.data.dataset.Dataset object at 0x7fb59a031898>, <torchtext.data.dataset.Dataset object at 0x7fb5991c93c8>)\n(<torchtext.data.dataset.Dataset object at 0x7fb59a014ef0>, <torchtext.data.dataset.Dataset object at 0x7fb5991c9ba8>)\n(<torchtext.data.dataset.Dataset object at 0x7fb59a031898>, <torchtext.data.dataset.Dataset object at 0x7fb5991c93c8>)\n"
     ]
    }
   ],
   "source": [
    "for i in iter_folds(data):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}