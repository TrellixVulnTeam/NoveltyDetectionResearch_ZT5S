{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datamodule import *\n",
    "from pytorch_lightning.callbacks import LearningRateLogger\n",
    "from pytorch_lightning.loggers import NeptuneLogger, TensorBoardLogger\n",
    "from pytorch_lightning.profiler import AdvancedProfiler\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from joblib import Memory\n",
    "import shutil\n",
    "import argparse\n",
    "from lang import *\n",
    "import joblib\n",
    "from pytorch_lightning import Callback\n",
    "from transformers import BertModel, BertTokenizer,DistilBertTokenizer\n",
    "from lang import *\n",
    "from snli.train_utils import SNLI_model, snli_glove_data_module, snli_bert_data_module,SwitchOptim\n",
    "from utils.keys import NEPTUNE_API\n",
    "from utils.helpers import seed_torch\n",
    "from utils.save_models import save_model,save_model_neptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = snli_bert_data_module(128)\n",
    "Lang = data_module.Lang\n",
    "embedding_matrix = None\n",
    "\n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "\n",
    "class Transformer_config:\n",
    "    embedding_dim = 768\n",
    "    initializer_range=0.02\n",
    "    max_len = 150\n",
    "    sub_enc_layer = 3\n",
    "    n_heads = 12\n",
    "    interaction = \"concat\"\n",
    "\n",
    "    def __init__(self, lang, embedding_matrix=None, **kwargs):\n",
    "        self.embedding_matrix = None\n",
    "        if lang.tokenizer_ == \"BERT\":\n",
    "            self.vocab_size = lang.vocab_size\n",
    "            self.padding_idx = lang.bert_tokenizer.vocab[\"[PAD]\"]\n",
    "        else:\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            self.vocab_size = lang.vocab_size_final()\n",
    "            self.padding_idx = lang.word2idx[lang.config.pad]\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _init_weights(module):\n",
    "    \"\"\" Initialize the weights \"\"\"\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "        # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=self.conf.vocab_size,\n",
    "            embedding_dim=self.conf.embedding_dim,\n",
    "            padding_idx=self.conf.padding_idx,\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Embedding(\n",
    "            num_embeddings=self.conf.max_len,\n",
    "            embedding_dim=self.conf.embedding_dim,\n",
    "        )\n",
    "        self.register_buffer(\"position_ids\", torch.arange(self.conf.max_len).expand((1, -1)))\n",
    "\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.conf.embedding_dim, nhead=self.conf.n_heads),\n",
    "            self.conf.sub_enc_layer,\n",
    "        )\n",
    "        self.LayerNorm = nn.LayerNorm(self.conf.embedding_dim)\n",
    "        self.pooler = nn.Linear(self.conf.embedding_dim, self.conf.embedding_dim)\n",
    "\n",
    "        self.translate = nn.Linear(self.conf.embedding_dim, self.conf.embedding_dim)\n",
    "        self.template = nn.Parameter(torch.zeros((1)), requires_grad=True)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def make_src_mask(self,src):\n",
    "        mask = src.transpose(0,1)==self.conf.padding_idx\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)\n",
    "        seq_length, N = x.shape\n",
    "        \n",
    "        position_ids = self.position_ids[:, :seq_length].expand(N,-1).transpose(0,1)\n",
    "\n",
    "        \n",
    "        emb = self.word_embedding(x)\n",
    "        pos_embedding = self.pos_embedding(position_ids)\n",
    "        emb = emb + pos_embedding\n",
    "\n",
    "        emb = self.LayerNorm(emb)\n",
    "        emb = self.dropout(emb)\n",
    "\n",
    "        mask = self.make_src_mask(x)\n",
    "        opt = self.transformer(emb, src_key_padding_mask = mask)\n",
    "        opt = self.pooler(opt)\n",
    "        opt = self.dropout(F.tanh(opt))\n",
    "        opt = self.translate(opt)\n",
    "        # opt = self.dropout(F.relu(opt))\n",
    "        return opt\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class Transformer_snli(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(Transformer_snli, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.encoder = TransformerEncoder(self.conf)\n",
    "\n",
    "        if self.conf.interaction == \"concat\":\n",
    "            final_dim = 2 * self.conf.embedding_dim\n",
    "            self.interact = self.interact_concat\n",
    "\n",
    "        elif self.conf.interaction == \"sum_prod\":\n",
    "            final_dim = 4 * self.conf.embedding_dim\n",
    "            self.interact = self.interact_sum_prod\n",
    "\n",
    "        self.cls = nn.Linear(final_dim, 3)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.apply(_init_weights)\n",
    "\n",
    "    def interact_concat(self, a, b):\n",
    "        return torch.cat([a, b], dim=2)\n",
    "\n",
    "    def interact_sum_prod(self, a, b):\n",
    "        return torch.cat([a, b, a + b, a * b], dim=2)\n",
    "\n",
    "    def forward(self, x0, x1):\n",
    "        x0_emb = self.encoder(x0)[:1, :, :]\n",
    "        x1_emb = self.encoder(x1)[:1, :, :]\n",
    "\n",
    "        conc = self.interact(x0_emb,x1_emb)\n",
    "        opt = self.cls(conc)\n",
    "\n",
    "        return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    \"optimizer_base\":{\n",
    "        \"optim\": \"adamw\",\n",
    "        \"lr\": 3e-4,\n",
    "        \"scheduler\": \"const\"\n",
    "        },\n",
    "    \"optimizer_tune\":{\n",
    "        \"optim\": \"adam\",\n",
    "        \"lr\": 3e-4,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"scheduler\": \"lambda\"\n",
    "    },\n",
    "    \"switch_epoch\":2,\n",
    "}\n",
    "\n",
    "conf_kwargs = {\n",
    "    \"batch_size\":128\n",
    "}\n",
    "\n",
    "EPOCHS=5\n",
    "\n",
    "conf = Transformer_config(Lang,**conf_kwargs)\n",
    "# model = Transformer_snli(conf)\n",
    "model = SNLI_model(Transformer_snli,conf,hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params | In sizes | Out sizes\n",
      "------------------------------------------------------------------\n",
      "0 | model | Transformer_snli | 41 M   | ?        | ?        \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f21abcc7f6a04a6ca1a87ff8b119919c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ""
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdce410ce1a549f8ba8233cacea87596"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Saving latest checkpoint..\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37b0760cac7547af95341b308d582d5e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_acc': tensor(0.6338), 'test_loss': tensor(0.8200, device='cuda:0')}\n--------------------------------------------------------------------------------\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_acc': 0.6338270902633667, 'test_loss': 0.8200086951255798}]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "tensorboard_logger = TensorBoardLogger(\"lightning_logs\")\n",
    "lr_logger = LearningRateLogger(logging_interval=\"step\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=EPOCHS,\n",
    "    progress_bar_refresh_rate=10,\n",
    "    profiler=False,\n",
    "    auto_lr_find=False,\n",
    "    callbacks=[lr_logger,SwitchOptim()],\n",
    "    logger=[tensorboard_logger],\n",
    "    row_log_interval=2,\n",
    "    # gradient_clip_val=0.5\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = data_module.train_dataloader()\n",
    "for i in dl:\n",
    "    a,b,c = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([128, 100])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([128, 100])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = torch.arange(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([128, 150])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "asd.expand(1,-1).expand(128,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([150])"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "asd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}