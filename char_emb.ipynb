{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn \n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/tag/stanford.py:183: DeprecationWarning: \nThe StanfordTokenizer will be deprecated in version 3.2.5.\nPlease use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n/usr/local/lib/python3.6/dist-packages/nltk/tag/stanford.py:149: DeprecationWarning: \nThe StanfordTokenizer will be deprecated in version 3.2.5.\nPlease use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "PADDING = \"<PAD>\"\n",
    "POS_Tagging = [PADDING, 'WP$', 'RBS', 'SYM', 'WRB', 'IN', 'VB', 'POS', 'TO', ':', '-RRB-', '$', 'MD', 'JJ', '#', 'CD', '``', 'JJR', 'NNP', \"''\", 'LS', 'VBP', 'VBD', 'FW', 'RBR', 'JJS', 'DT', 'VBG', 'RP', 'NNS', 'RB', 'PDT', 'PRP$', '.', 'XX', 'NNPS', 'UH', 'EX', 'NN', 'WDT', 'VBN', 'VBZ', 'CC', ',', '-LRB-', 'PRP', 'WP']\n",
    "POS_dict = {pos:i for i, pos in enumerate(POS_Tagging)}\n",
    "\n",
    "stemmer = nltk.SnowballStemmer('english')\n",
    "\n",
    "tt = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "\n",
    "nst = StanfordNERTagger('stanford-ner-2020-11-17/classifiers/english.muc.7class.distsim.crf.ser.gz', 'stanford-ner-2020-11-17/stanford-ner-4.2.0.jar',encoding='utf-8')\n",
    "\n",
    "\n",
    "pst = StanfordPOSTagger('stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger', \\\n",
    "                    'stanford-postagger-full-2020-11-17/stanford-postagger.jar')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_exact_match(token1, token2):\n",
    "    token1 = token1.lower()\n",
    "    token2 = token2.lower()\n",
    "    \n",
    "    token1_stem = stemmer.stem(token1)\n",
    "\n",
    "    if token1 == token2:\n",
    "        return True\n",
    "    \n",
    "    for synsets in wn.synsets(token2):\n",
    "        for lemma in synsets.lemma_names():\n",
    "            if token1_stem == stemmer.stem(lemma):\n",
    "                return True\n",
    "    \n",
    "    if token1 == \"n't\" and token2 == \"not\":\n",
    "        return True\n",
    "    elif token1 == \"not\" and token2 == \"n't\":\n",
    "        return True\n",
    "    elif token1_stem == stemmer.stem(token2):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_antonyms(token1, token2):\n",
    "    token1 = token1.lower()\n",
    "    token2 = token2.lower()\n",
    "    token1_stem = stemmer.stem(token1)\n",
    "    antonym_lists_for_token2 = []\n",
    "    for synsets in wn.synsets(token2):\n",
    "        for lemma_synsets in [wn.synsets(l) for l in synsets.lemma_names()]:\n",
    "            for lemma_syn in lemma_synsets:\n",
    "                for lemma in lemma_syn.lemmas():\n",
    "                    for antonym in lemma.antonyms():\n",
    "                        antonym_lists_for_token2.append(antonym.name())\n",
    "                        # if token1_stem == stemmer.stem(antonym.name()):\n",
    "                        #     return True \n",
    "    antonym_lists_for_token2 = list(set(antonym_lists_for_token2))\n",
    "    for atnm in antonym_lists_for_token2:\n",
    "        if token1_stem == stemmer.stem(atnm):\n",
    "            return True\n",
    "    return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang import *\n",
    "from datamodule import *\n",
    "from snli.train_utils import *\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from datamodule import *\n",
    "from pytorch_lightning.callbacks import LearningRateLogger\n",
    "from snli.attn_enc.attn_enc import *\n",
    "from pytorch_lightning.loggers import NeptuneLogger, TensorBoardLogger\n",
    "from pytorch_lightning.profiler import AdvancedProfiler\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = snli_bert_data_module(char_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Attn_Encoder_conf:\n",
    "    embedding_dim = 300\n",
    "    hidden_size = 300\n",
    "    fcs = 1\n",
    "    num_layers = 2\n",
    "    dropout = 0.1\n",
    "    opt_labels = 3\n",
    "    bidirectional = True\n",
    "    attn_type = \"dot\"\n",
    "    attention_layer_param = 100\n",
    "    activation = \"tanh\"\n",
    "    freeze_embedding = False\n",
    "    char_embedding_size = 50\n",
    "\n",
    "    def __init__(self, lang, embedding_matrix=None, **kwargs):\n",
    "        self.embedding_matrix = None\n",
    "        self.char_emb = lang.char_emb\n",
    "        self.char_vocab_size = lang.char_vocab_size\n",
    "        self.char_word_len = lang.char_emb_max_len\n",
    "\n",
    "        if lang.tokenizer_ == \"BERT\":\n",
    "            self.vocab_size = lang.vocab_size\n",
    "            self.padding_idx = lang.bert_tokenizer.vocab[\"[PAD]\"]\n",
    "        else:\n",
    "            self.embedding_matrix = embedding_matrix\n",
    "            self.vocab_size = lang.vocab_size_final()\n",
    "            self.padding_idx = lang.word2idx[lang.config.pad]\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Ws = nn.Linear(\n",
    "            (2 if conf.bidirectional else 1) * conf.hidden_size,\n",
    "            conf.attention_layer_param,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.Wa = nn.Linear(conf.attention_layer_param, 1, bias=False)\n",
    "\n",
    "    def forward(self, hid):\n",
    "        opt = self.Ws(hid)\n",
    "        opt = F.tanh(opt)\n",
    "        opt = self.Wa(opt)\n",
    "        opt = F.softmax(opt)\n",
    "        return opt\n",
    "\n",
    "\n",
    "class Attn_Encoder(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(Attn_Encoder, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.conf.vocab_size,\n",
    "            embedding_dim=self.conf.embedding_dim,\n",
    "            padding_idx=self.conf.padding_idx,\n",
    "        )\n",
    "        if self.conf.char_emb:\n",
    "            self.char_embedding = nn.Embedding(\n",
    "                num_embeddings=self.conf.char_vocab_size,\n",
    "                embedding_dim=self.conf.char_embedding_size,\n",
    "                padding_idx=0\n",
    "            )\n",
    "            self.char_cnn = nn.Conv2d(self.conf.char_word_len,self.conf.char_embedding_size , (1, 6), stride=(1, 1), padding=0, bias=True)\n",
    "        self.translate = nn.Linear(\n",
    "            self.conf.embedding_dim+(self.conf.char_embedding_size if self.conf.char_emb else 0), self.conf.hidden_size\n",
    "        )  # make (300,..) if not working\n",
    "        if self.conf.activation.lower() == \"relu\".lower():\n",
    "            self.act = nn.ReLU()\n",
    "        elif self.conf.activation.lower() == \"tanh\".lower():\n",
    "            self.act = nn.Tanh()\n",
    "        elif self.conf.activation.lower() == \"leakyrelu\".lower():\n",
    "            self.act = nn.LeakyReLU()\n",
    "        if isinstance(self.conf.embedding_matrix, np.ndarray):\n",
    "            self.embedding.from_pretrained(\n",
    "                torch.tensor(self.conf.embedding_matrix),\n",
    "                freeze=self.conf.freeze_embedding,\n",
    "            )\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size=self.conf.hidden_size,\n",
    "            hidden_size=self.conf.hidden_size,\n",
    "            num_layers=self.conf.num_layers,\n",
    "            bidirectional=self.conf.bidirectional,\n",
    "        )\n",
    "        self.attention = Attention(conf)\n",
    "\n",
    "    def char_embedding_forward(self,x):\n",
    "        #X - [batch_size, seq_len, char_emb_size])\n",
    "        batch_size, seq_len, char_emb_size= x.shape\n",
    "        x = x.view(-1,char_emb_size)\n",
    "        x = self.char_embedding(x) #(batch_size * seq_len, char_emb_size, emb_size)\n",
    "        x = x.view(batch_size, -1, seq_len, char_emb_size)\n",
    "        x = x.permute(0,3,2,1)\n",
    "        x = self.char_cnn(x)\n",
    "        x = torch.max(F.relu(x), 3)[0]\n",
    "        return x.view(-1,seq_len,self.conf.char_embedding_size)\n",
    "\n",
    "\n",
    "    def forward(self, inp, char_vec = None):\n",
    "        batch_size = inp.shape[0]\n",
    "        embedded = self.embedding(inp)\n",
    "        if char_vec!=None:\n",
    "            char_emb = self.char_embedding_forward(char_vec)\n",
    "            embedded = torch.cat([embedded,char_emb],dim=2)\n",
    "\n",
    "        embedded = self.translate(embedded)\n",
    "        embedded = self.act(embedded)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        all_, (hid, cell) = self.lstm_layer(embedded)\n",
    "\n",
    "        attn = self.attention(all_)\n",
    "\n",
    "        cont = torch.bmm(all_.permute(1, 2, 0), attn.permute(1, 0, 2)).permute(2, 0, 1)\n",
    "        return cont\n",
    "\n",
    "\n",
    "class Attn_encoder_snli(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(Attn_encoder_snli, self).__init__()\n",
    "        self.conf = conf\n",
    "        self.encoder = Attn_Encoder(conf)\n",
    "        self.fc_in = nn.Linear(\n",
    "            (2 if conf.bidirectional else 1) * 4 * self.conf.hidden_size,\n",
    "            self.conf.hidden_size,\n",
    "        )\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(self.conf.hidden_size, self.conf.hidden_size)\n",
    "                for i in range(self.conf.fcs)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(self.conf.hidden_size, self.conf.opt_labels)\n",
    "        if self.conf.activation.lower() == \"relu\".lower():\n",
    "            self.act = nn.ReLU()\n",
    "        elif self.conf.activation.lower() == \"tanh\".lower():\n",
    "            self.act = nn.Tanh()\n",
    "        elif self.conf.activation.lower() == \"leakyrelu\".lower():\n",
    "            self.act = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.dropout = nn.Dropout(p=self.conf.dropout)\n",
    "\n",
    "    def forward(self, x0, x1, x0_char_vec = None, x1_char_vec = None):\n",
    "        x0_enc = self.encoder(x0.long(),char_vec = x0_char_vec)\n",
    "        x0_enc = self.dropout(x0_enc)\n",
    "        x1_enc = self.encoder(x1.long(),char_vec = x1_char_vec)\n",
    "        x1_enc = self.dropout(x1_enc)\n",
    "        cont = torch.cat(\n",
    "            [x0_enc, x1_enc, torch.abs(x0_enc - x1_enc), x0_enc * x1_enc], dim=2\n",
    "        )\n",
    "        opt = self.fc_in(cont)\n",
    "        opt = self.dropout(opt)\n",
    "        for fc in self.fcs:\n",
    "            opt = fc(opt)\n",
    "            opt = self.dropout(opt)\n",
    "            opt = self.act(opt)\n",
    "        opt = self.fc_out(opt)\n",
    "        return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = datamodule.Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_kwargs = {\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.10018262692246818,\n",
    "        \"embedding_dim\": 300,\n",
    "        \"hidden_size\": 400,\n",
    "        \"attention_layer_param\": 250,\n",
    "        \"bidirectional\": True,\n",
    "        \"freeze_embedding\": False,\n",
    "        \"activation\": \"tanh\",\n",
    "        \"fcs\": 1,\n",
    "        \"glove\": False,\n",
    "        \"batch_size\": 128,\n",
    "        \"max_len\": 110,\n",
    "    }\n",
    "\n",
    "hparams = {\n",
    "    \"optimizer_base\": {\n",
    "        \"optim\": \"adamw\",\n",
    "        \"lr\": 0.0010039910781394373,\n",
    "        \"scheduler\": \"const\",\n",
    "    },\n",
    "    \"optimizer_tune\": {\n",
    "        \"optim\": \"adam\",\n",
    "        \"lr\": 0.0010039910781394373,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"scheduler\": \"lambda\",\n",
    "    },\n",
    "    \"switch_epoch\": 5,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "model(i[0],i[2],x0_char_vec = i[1],x1_char_vec = i[3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model_conf = Attn_Encoder_conf(lang,None, **conf_kwargs)\n",
    "model = SNLI_char_emb(Attn_encoder_snli, model_conf, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datamodule.train_dataloader():\n",
    "    a,b,c,d,e = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model(a.cuda(),c.cuda(),b.cuda(),d.cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | Attn_encoder_snli | 17 M  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad5eb2c6f7a74851811d4560977dfe05"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ""
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0671548dcfde44cb870877fa90d11aee"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Saving latest checkpoint..\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "\n",
    "\n",
    "tensorboard_logger = TensorBoardLogger(\"lightning_logs\")\n",
    "lr_logger = LearningRateLogger(logging_interval=\"step\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=1,\n",
    "    progress_bar_refresh_rate=10,\n",
    "    profiler=False,\n",
    "    auto_lr_find=False,\n",
    "    callbacks=[lr_logger, SwitchOptim()],\n",
    "    logger=[tensorboard_logger],\n",
    "    row_log_interval=2,\n",
    ")\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}