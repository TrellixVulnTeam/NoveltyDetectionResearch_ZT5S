{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_padded_index_sequences(datasets):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding. \n",
    "    \"\"\"\n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        string = re.sub(r'\\(|\\)', '', string)\n",
    "        return string.split()\n",
    "    \n",
    "\n",
    "    word_counter = collections.Counter()\n",
    "    char_counter = collections.Counter()\n",
    "\n",
    "    for example in tqdm(dataset):\n",
    "        s1_tokenize = tokenize(example[0])\n",
    "        s2_tokenize = tokenize(example[1])\n",
    "\n",
    "        word_counter.update(s1_tokenize)\n",
    "        word_counter.update(s2_tokenize)\n",
    "\n",
    "        for i, word in enumerate(s1_tokenize):\n",
    "            char_counter.update([c for c in word])\n",
    "        for word in s2_tokenize:\n",
    "            char_counter.update([c for c in word])\n",
    "\n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    if config.embedding_replacing_rare_word_with_UNK: \n",
    "        vocabulary = [PADDING, \"<UNK>\"] + vocabulary\n",
    "    else:\n",
    "        vocabulary = [PADDING] + vocabulary\n",
    "    \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "    char_vocab = set([char for char in char_counter])\n",
    "    char_vocab = list(char_vocab)\n",
    "    char_vocab = [PADDING] + char_vocab\n",
    "    char_indices = dict(zip(char_vocab, range(len(char_vocab))))\n",
    "    indices_to_char = {v: k for k, v in char_indices.items()}\n",
    "    \n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in tqdm(dataset):\n",
    "            for sentence in [, 'sentence2_binary_parse']:\n",
    "                example[sentence + '_index_sequence'] = np.zeros((FIXED_PARAMETERS[\"seq_length\"]), dtype=np.int32)\n",
    "                example[sentence + '_inverse_term_frequency'] = np.zeros((FIXED_PARAMETERS[\"seq_length\"]), dtype=np.float32)\n",
    "\n",
    "                token_sequence = tokenize(example[sentence])\n",
    "                padding = FIXED_PARAMETERS[\"seq_length\"] - len(token_sequence)\n",
    "                      \n",
    "                for i in range(FIXED_PARAMETERS[\"seq_length\"]):\n",
    "                    if i >= len(token_sequence):\n",
    "                        index = word_indices[PADDING]\n",
    "                        itf = 0\n",
    "                    else:\n",
    "                        if config.embedding_replacing_rare_word_with_UNK:\n",
    "                            index = word_indices[token_sequence[i]] if word_counter[token_sequence[i]] >= config.UNK_threshold else word_indices[\"<UNK>\"]\n",
    "                        else:\n",
    "                            index = word_indices[token_sequence[i]]\n",
    "                        itf = 1 / (word_counter[token_sequence[i]] + 1)\n",
    "                    example[sentence + '_index_sequence'][i] = index\n",
    "                    \n",
    "                    example[sentence + '_inverse_term_frequency'][i] = itf\n",
    "                \n",
    "                example[sentence + '_char_index'] = np.zeros((FIXED_PARAMETERS[\"seq_length\"], config.char_in_word_size), dtype=np.int32)\n",
    "                for i in range(FIXED_PARAMETERS[\"seq_length\"]):\n",
    "                    if i >= len(token_sequence):\n",
    "                        continue\n",
    "                    else:\n",
    "                        chars = [c for c in token_sequence[i]]\n",
    "                        for j in range(config.char_in_word_size):\n",
    "                            if j >= (len(chars)):\n",
    "                                break\n",
    "                            else:\n",
    "                                index = char_indices[chars[j]]\n",
    "                            example[sentence + '_char_index'][i,j] = index \n",
    "    \n",
    "\n",
    "    return indices_to_words, word_indices, char_indices, indices_to_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn \n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = \"<PAD>\"\n",
    "POS_Tagging = [PADDING, 'WP$', 'RBS', 'SYM', 'WRB', 'IN', 'VB', 'POS', 'TO', ':', '-RRB-', '$', 'MD', 'JJ', '#', 'CD', '``', 'JJR', 'NNP', \"''\", 'LS', 'VBP', 'VBD', 'FW', 'RBR', 'JJS', 'DT', 'VBG', 'RP', 'NNS', 'RB', 'PDT', 'PRP$', '.', 'XX', 'NNPS', 'UH', 'EX', 'NN', 'WDT', 'VBN', 'VBZ', 'CC', ',', '-LRB-', 'PRP', 'WP']\n",
    "POS_dict = {pos:i for i, pos in enumerate(POS_Tagging)}\n",
    "\n",
    "stemmer = nltk.SnowballStemmer('english')\n",
    "\n",
    "tt = nltk.tokenize.treebank.TreebankWordTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_exact_match(token1, token2):\n",
    "    token1 = token1.lower()\n",
    "    token2 = token2.lower()\n",
    "    \n",
    "    token1_stem = stemmer.stem(token1)\n",
    "\n",
    "    if token1 == token2:\n",
    "        return True\n",
    "    \n",
    "    for synsets in wn.synsets(token2):\n",
    "        for lemma in synsets.lemma_names():\n",
    "            if token1_stem == stemmer.stem(lemma):\n",
    "                return True\n",
    "    \n",
    "    if token1 == \"n't\" and token2 == \"not\":\n",
    "        return True\n",
    "    elif token1 == \"not\" and token2 == \"n't\":\n",
    "        return True\n",
    "    elif token1_stem == stemmer.stem(token2):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_antonyms(token1, token2):\n",
    "    token1 = token1.lower()\n",
    "    token2 = token2.lower()\n",
    "    token1_stem = stemmer.stem(token1)\n",
    "    antonym_lists_for_token2 = []\n",
    "    for synsets in wn.synsets(token2):\n",
    "        for lemma_synsets in [wn.synsets(l) for l in synsets.lemma_names()]:\n",
    "            for lemma_syn in lemma_synsets:\n",
    "                for lemma in lemma_syn.lemmas():\n",
    "                    for antonym in lemma.antonyms():\n",
    "                        antonym_lists_for_token2.append(antonym.name())\n",
    "                        # if token1_stem == stemmer.stem(antonym.name()):\n",
    "                        #     return True \n",
    "    antonym_lists_for_token2 = list(set(antonym_lists_for_token2))\n",
    "    for atnm in antonym_lists_for_token2:\n",
    "        if token1_stem == stemmer.stem(atnm):\n",
    "            return True\n",
    "    return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SNLIDataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('A person on a horse jumps over a broken down airplane.', 'A person is training his horse for a competition.', tensor(2))\n"
     ]
    }
   ],
   "source": [
    "for i,e in enumerate(data):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 549360/549360 [00:42<00:00, 13078.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract vocabulary\n",
    "def tokenize(string):\n",
    "    string = re.sub(r'\\(|\\)', '', string)\n",
    "    return string.split()\n",
    "\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "char_counter = collections.Counter()\n",
    "\n",
    "for example in tqdm(data):\n",
    "    s1_tokenize = tokenize(example[0])\n",
    "    s2_tokenize = tokenize(example[1])\n",
    "\n",
    "    word_counter.update(s1_tokenize)\n",
    "    word_counter.update(s2_tokenize)\n",
    "\n",
    "    for i, word in enumerate(s1_tokenize):\n",
    "        char_counter.update([c for c in word])\n",
    "    for word in s2_tokenize:\n",
    "        char_counter.update([c for c in word])\n",
    "\n",
    "vocabulary = set([word for word in word_counter])\n",
    "vocabulary = list(vocabulary)\n",
    "vocabulary = [PADDING, \"<UNK>\"] + vocabulary\n",
    "\n",
    "\n",
    "word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "char_vocab = set([char for char in char_counter])\n",
    "char_vocab = list(char_vocab)\n",
    "char_vocab = [PADDING] + char_vocab\n",
    "char_indices = dict(zip(char_vocab, range(len(char_vocab))))\n",
    "indices_to_char = {v: k for k, v in char_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 549360/549360 [00:52<00:00, 10524.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for example in tqdm(data):\n",
    "    indx_seq = np.zeros((12), dtype=np.int32)\n",
    "    tfidf = np.zeros((12), dtype=np.float32)\n",
    "\n",
    "    for sentence in [0,1]:\n",
    "        token_sequence = tokenize(example[sentence])\n",
    "        padding = 12 - len(token_sequence)\n",
    "        \n",
    "        for i in range(12):\n",
    "            if i >= len(token_sequence):\n",
    "                index = word_indices[PADDING]\n",
    "                itf = 0\n",
    "            else:\n",
    "                \n",
    "                index = word_indices[token_sequence[i]] if word_counter[token_sequence[i]] >= 3 else word_indices[\"<UNK>\"]\n",
    "                itf = 1 / (word_counter[token_sequence[i]] + 1)\n",
    "            indx_seq[i] = index\n",
    "            \n",
    "            tfidf[i] = itf\n",
    "        \n",
    "        char_index = np.zeros((12, 8), dtype=np.int32)\n",
    "        for i in range(12):\n",
    "            if i >= len(token_sequence):\n",
    "                continue\n",
    "            else:\n",
    "                chars = [c for c in token_sequence[i]]\n",
    "                for j in range(8):\n",
    "                        if j >= (len(chars)):\n",
    "                            break\n",
    "                        else:\n",
    "                            index = char_indices[chars[j]]\n",
    "                        char_index[i,j] = index \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[12, 19,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 73, 25,  0,  0,  0,  0,  0],\n",
       "       [15, 25, 35, 43,  1,  2, 18, 43],\n",
       "       [15, 13, 43, 25,  0,  0,  0,  0],\n",
       "       [64, 35,  1, 25, 34,  0,  0,  0],\n",
       "       [ 1, 73, 25, 34, 25,  0,  0,  0],\n",
       "       [ 2, 28,  0,  0,  0,  0,  0,  0],\n",
       "       [35,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [72, 35, 19,  0,  0,  0,  0,  0],\n",
       "       [ 2, 19,  0,  0,  0,  0,  0,  0],\n",
       "       [35,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [15,  5, 62, 46, 28, 43,  2,  1]], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "char_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([12, 19,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "char_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 549360/549360 [02:33<00:00, 3569.42it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(data):\n",
    "    s1 = nltk.word_tokenize(i[0])\n",
    "    s2 = nltk.word_tokenize(i[1])\n",
    "\n",
    "    lens = []\n",
    "    lens += [len(i) for i in s1]\n",
    "    lens += [len(i) for i in s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pst' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0772316ed604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0ms1_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pst' is not defined"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    s1_pos = pst.tag(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nst = StanfordNERTagger('stanford-ner-2020-11-17/classifiers/english.muc.7class.distsim.crf.ser.gz', 'stanford-ner-2020-11-17/stanford-ner-4.2.0.jar',encoding='utf-8')\n",
    "pst = StanfordPOSTagger('/home/users/yichen.gong/Stanford/stanford-postagger-2014-08-27/models/english-bidirectional-distsim.tagger', \\\n",
    "                    '/home/users/yichen.gong/Stanford/stanford-postagger-2014-08-27/stanford-postagger.jar')\n"
   ]
  }
 ]
}