{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Errors :  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "wsj =\"./dataset/trec/wsj\"\n",
    "ap =\"./dataset/trec/ap\"\n",
    "wsj_files = glob.glob(wsj+\"/*\")\n",
    "ap_files = glob.glob(ap+\"/*\")\n",
    "\n",
    "docs_json = {}\n",
    "errors = 0\n",
    "for wsj_file in wsj_files:\n",
    "    with open(wsj_file,'r') as f:\n",
    "        txt = f.read()\n",
    "    docs = [i.split(\"<DOC>\")[1] for i in filter(lambda x:len(x)>10,txt.split('</DOC>'))]\n",
    "\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            id = doc.split(\"<DOCNO>\")[1].split('</DOCNO>')[0]\n",
    "            text = doc.split(\"<TEXT>\")[1].split('</TEXT>')[0]\n",
    "            docs_json[id] = text\n",
    "        except:\n",
    "            errors+=1\n",
    "\n",
    "for ap_file in ap_files:\n",
    "    with open(ap_file,'r',encoding=\"latin-1\") as f:\n",
    "        txt = f.read()\n",
    "    docs = [i.split(\"<DOC>\")[1] for i in filter(lambda x:len(x)>10,txt.split('</DOC>'))]\n",
    "\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            id = doc.split(\"<DOCNO>\")[1].split('</DOCNO>')[0]\n",
    "            text = doc.split(\"<TEXT>\")[1].split('</TEXT>')[0]\n",
    "            docs_json[id] = text\n",
    "        except:\n",
    "            errors+=1\n",
    "print(\"Reading APWSJ dataset, Errors : \",errors)\n",
    "\n",
    "docs_json = {k.strip():v.strip() for k,v in docs_json.items()}"
   ]
  },
  {
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "topic_to_doc_file = \"./dataset/apwsj/NoveltyData/apwsj.qrels\"\n",
    "with open(topic_to_doc_file,'r') as f:\n",
    "    topic_to_doc = f.read()\n",
    "topic_doc = [(i.split(' 0 ')[1][:-2],i.split(' 0 ')[0]) for i in topic_to_doc.split(\"\\n\")]\n",
    "topics = \"q101, q102, q103, q104, q105, q106, q107, q108, q109, q111, q112, q113, q114, q115, q116, q117, q118, q119, q120, q121, q123, q124, q125, q127, q128, q129, q132, q135, q136, q137, q138, q139, q141\"\n",
    "topic_list = topics.split(\", \")\n",
    "filterd_docid = [(k,v) for k,v in topic_doc if v in topic_list]\n",
    "\n",
    "def crawl(red_dict,doc,crawled):\n",
    "\tans=[]\n",
    "\tfor cdoc in red_dict[doc]:\n",
    "\t\tans.append(cdoc)\n",
    "\t\tif crawled[cdoc]==0:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tred_dict[cdoc]=crawl(red_dict,cdoc,crawled)\n",
    "\t\t\t\tcrawled[cdoc]=1\n",
    "\t\t\t\tans+=red_dict[cdoc]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tcrawled[cdoc]=1\n",
    "\treturn ans\n",
    "\n",
    "wf = \"./dataset/apwsj/redundancy_list_without_partially_redundant.txt\"\n",
    "topics_allowed=\"q101, q102, q103, q104, q105, q106, q107, q108, q109, q111, q112, q113, q114, q115, q116, q117, q118, q119, q120, q121, q123, q124, q125, q127, q128, q129, q132, q135, q136, q137, q138, q139, q141\"\n",
    "topics_allowed=topics_allowed.split(\", \")\n",
    "red_dict = dict()\n",
    "allow_partially_redundant = 1\n",
    "for line in open(\"./dataset/apwsj/NoveltyData/redundancy.apwsj.result\",\"r\"):\n",
    "\ttokens = line.split()\n",
    "\tif tokens[2]==\"?\":\n",
    "\t\tif allow_partially_redundant==1:\n",
    "\t\t\tred_dict[tokens[0]+\"/\"+tokens[1]]=[tokens[0]+\"/\"+i for i in tokens[3:]]\n",
    "\telse:\n",
    "\t\tred_dict[tokens[0]+\"/\"+tokens[1]]=[tokens[0]+\"/\"+i for i in tokens[2:]]\n",
    "crawled=defaultdict(int)\n",
    "for doc in red_dict:\n",
    "\tif crawled[doc]==0:\n",
    "\t\tred_dict[doc]=crawl(red_dict,doc,crawled)\n",
    "\t\tcrawled[doc]=1\n",
    "with open(wf,\"w\") as f:\n",
    "\tfor doc in red_dict:\n",
    "\t\tif doc.split(\"/\")[0] in topics_allowed:\n",
    "\t\t\tf.write(\" \".join(doc.split(\"/\")+[i.split(\"/\")[1] for i in red_dict[doc]])+\"\\n\")\n",
    "\n",
    "write_file=\"./dataset/apwsj/novel_list_without_partially_redundant.txt\"\n",
    "topics = topic_list\n",
    "doc_topic_dict=defaultdict(list)\n",
    "\n",
    "for i in topic_doc:\n",
    "    doc_topic_dict[i[0]].append(i[1])\n",
    "docs_sorted = open(\"./dataset/apwsj/NoveltyData/apwsj88-90.rel.docno.sorted\",\"r\").read().splitlines()\n",
    "sorted_doc_topic_dict = defaultdict(list)\n",
    "for doc in docs_sorted:\n",
    "\tif len(doc_topic_dict[doc])>0:\n",
    "\t\tfor t in doc_topic_dict[doc]:\n",
    "\t\t\tsorted_doc_topic_dict[t].append(doc)\n",
    "redundant_dict= defaultdict(lambda: defaultdict(int))\n",
    "for line in open(\"./dataset/apwsj/redundancy_list_without_partially_redundant.txt\",\"r\"):\n",
    "\ttokens=line.split()\n",
    "\tredundant_dict[tokens[0]][tokens[1]]=1\n",
    "novel_list=[]\n",
    "for topic in topics:\n",
    "\tif topic in topics_allowed:\n",
    "\t\tfor i in range(len(sorted_doc_topic_dict[topic])):\n",
    "\t\t\tif redundant_dict[topic][sorted_doc_topic_dict[topic][i]]!=1:\n",
    "\t\t\t\tif i>0:\n",
    "\t\t\t\t\t# take at most 5 latest docs in case of novel\n",
    "\t\t\t\t\tnovel_list.append(\" \".join([topic,sorted_doc_topic_dict[topic][i]]+sorted_doc_topic_dict[topic][max(0,i-5):i]))\n",
    "with open(write_file,\"w\") as f:\n",
    "\tf.write(\"\\n\".join(novel_list))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novel cases\n",
    "novel_docs=\"./dataset/apwsj/novel_list_without_partially_redundant.txt\"\n",
    "with open(novel_docs,'r') as f:\n",
    "    novel_doc_list = [i.split() for i in f.read().split(\"\\n\")]\n",
    "# Redundant cases\n",
    "red_docs=\"./dataset/apwsj/redundancy_list_without_partially_redundant.txt\"\n",
    "with open(red_docs,'r') as f:\n",
    "    red_doc_list = [i.split() for i in f.read().split(\"\\n\")]\n",
    "red_doc_list = filter(lambda x:len(x)>0,red_doc_list)\n",
    "novel_doc_list = filter(lambda x:len(x)>0,novel_doc_list)\n",
    "\n",
    "dataset = []\n",
    "s_not_found=0\n",
    "t_not_found=0\n",
    "for i in novel_doc_list:\n",
    "    target_id = i[1]\n",
    "    source_ids = i[2:]\n",
    "    if target_id in docs_json.keys():\n",
    "        data_inst = {}\n",
    "        data_inst[\"target\"] = docs_json[target_id]\n",
    "        data_inst[\"source\"] = \"\"\n",
    "        for source_id in source_ids:\n",
    "            if source_id in docs_json.keys():\n",
    "                data_inst[\"source\"]+=docs_json[source_id]+'\\n'\n",
    "        data_inst[\"label\"] = 1\n",
    "    if data_inst[\"source\"]!=\"\":\n",
    "        dataset.append(data_inst)\n",
    "\n",
    "\n",
    "for i in red_doc_list:\n",
    "    target_id = i[1]\n",
    "    source_ids = i[2:]\n",
    "    if target_id in docs_json.keys():\n",
    "        data_inst = {}\n",
    "        data_inst[\"target\"] = docs_json[target_id]\n",
    "        data_inst[\"source\"] = \"\"\n",
    "        for source_id in source_ids:\n",
    "            if source_id in docs_json.keys():\n",
    "                data_inst[\"source\"]+=docs_json[source_id]+'\\n'\n",
    "        data_inst[\"label\"] = 0\n",
    "    if data_inst[\"source\"]!=\"\":\n",
    "        dataset.append(data_inst)\n",
    "\n",
    "dataset_json = {}\n",
    "for i in range(len(dataset)):\n",
    "    dataset_json[i] = dataset[i]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataset_path=\"./dataset/apwsj/apwsj_dataset.json\"\n",
    "with open(dataset_path,'w') as f:\n",
    "    json.dump(dataset_json,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "apwsj_data_path = \"./dataset/apwsj/apwsj_dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(apwsj_data_path,'r') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamodule import APWSJDataModule\n",
    "from utils import load_bilstm_encoder, load_attn_encoder\n",
    "encoder, Lang = load_attn_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = APWSJDataModule(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Size  5634\ntrain_samples:  4507\n"
     ]
    }
   ],
   "source": [
    "data_module.prepare_data(Lang,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(168, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "data_module.APWSJ_data.__getitem__(100)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import APWSJDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = APWSJDataset()\n",
    "# dataset1 = APWSJDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.org = [\n",
    "            list(\n",
    "                filter(\n",
    "                    lambda x: x != \"\" and x != \" \",\n",
    "                    [lang.preprocess_sentence(j) for j in nltk.sent_tokenize(i)],\n",
    "                )\n",
    "            )\n",
    "            for i in dataset.org\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = Lang\n",
    "\n",
    "dataset.encode_lang(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "111\n692\n693\n1069\n1274\n2602\n3484\n3485\n4472\n4473\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "for i in dataset.org:\n",
    "    x+=1\n",
    "    if i.shape[0]==0:\n",
    "        print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}