{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "import joblib\n",
    "import pickle\n",
    "import argparse\n",
    "from lang import *\n",
    "from novelty.han.han_novelty import *\n",
    "from snli.bilstm.bilstm import *\n",
    "from snli.attn_enc.attn_enc import *\n",
    "from novelty.train_utils import *\n",
    "import shutil\n",
    "from utils import (\n",
    "    load_bilstm_encoder,\n",
    "    load_attn_encoder,\n",
    "    load_han_clf_encoder,\n",
    "    load_han_reg_encoder,\n",
    "    reset_model,\n",
    "    seed_torch,\n",
    ")\n",
    "import numpy as np\n",
    "from novelty.train_utils import *\n",
    "from datamodule import *\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils.keys import NEPTUNE_API\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "import neptune\n",
    "from pytorch_lightning.metrics import Accuracy,F1,Precision,Recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_module, optimizer, device):\n",
    "    model.train()\n",
    "    loss_values = []\n",
    "    for batch in tqdm(data_module.train_dataloader()):\n",
    "        x0, x1, y = batch\n",
    "        model.zero_grad()\n",
    "        opt = model(x0.to(device), x1.to(device)).squeeze(1)\n",
    "        loss = F.cross_entropy(opt, y.to(device))\n",
    "        loss.backward()\n",
    "        loss_values.append(loss.cpu().item())\n",
    "        optimizer.step()\n",
    "    return np.mean(loss_values)\n",
    "\n",
    "def evaluate(model, data_module, device):\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    precision_values = []\n",
    "    recall_values = []\n",
    "    f1_values = []\n",
    "    acc_metric = Accuracy(num_classes=2)\n",
    "    prec_metric = Precision(num_classes=2)\n",
    "    recall_metric = Recall(num_classes=2)\n",
    "    F1_metric = F1(num_classes=2)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_module.test_dataloader()):\n",
    "            x0, x1, y = batch\n",
    "            model.zero_grad()\n",
    "            opt = model(x0.to(device), x1.to(device)).squeeze(1)\n",
    "            loss = F.cross_entropy(opt, y.to(device))\n",
    "            pred = F.softmax(opt)\n",
    "            loss_values.append(loss.cpu().item())\n",
    "            accuracy_values.append(acc_metric(pred.argmax(1).cpu(),y.cpu()))\n",
    "            precision_values.append(prec_metric(pred.argmax(1).cpu(),y.cpu()))\n",
    "            recall_values.append(recall_metric(pred.argmax(1).cpu(),y.cpu()))\n",
    "            f1_values.append(F1_metric(pred.argmax(1).cpu(),y.cpu()))\n",
    "\n",
    "    print(accuracy_values)\n",
    "    return (\n",
    "        np.mean(loss_values),\n",
    "        np.mean(accuracy_values),\n",
    "        np.mean(precision_values),\n",
    "        np.mean(recall_values),\n",
    "        np.mean(f1_values),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ = 'clf'\n",
    "if encoder_ == \"reg\":\n",
    "    encoder, Lang = load_han_reg_encoder()\n",
    "elif encoder_ == \"clf\":\n",
    "    encoder, Lang = load_han_clf_encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "webis = False\n",
    "data_module = (\n",
    "    WebisDataModule(batch_size=32, cross_val=True)\n",
    "    if webis\n",
    "    else DLNDDataModule(batch_size=32, cross_val=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.prepare_data(Lang, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"optim\": \"adamw\",\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"lr\": 0.00010869262115700171,\n",
    "    \"scheduler\": \"lambda\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = HAN_Novelty_conf(encoder, **hparams)\n",
    "model = HAN_Novelty(model_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=hparams[\"lr\"],weight_decay = hparams[\"weight_decay\"])\n",
    "init_state = copy.deepcopy(model.state_dict())\n",
    "init_state_opt = copy.deepcopy(optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "HAN_Novelty(\n",
       "  (doc_encoder): HAN(\n",
       "    (encoder): Attn_Encoder(\n",
       "      (embedding): Embedding(30522, 300, padding_idx=0)\n",
       "      (translate): Linear(in_features=300, out_features=400, bias=True)\n",
       "      (act): Tanh()\n",
       "      (lstm_layer): LSTM(400, 400, num_layers=2, bidirectional=True)\n",
       "      (attention): Attention(\n",
       "        (Ws): Linear(in_features=800, out_features=100, bias=False)\n",
       "        (Wa): Linear(in_features=100, out_features=1, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (translate): Linear(in_features=800, out_features=300, bias=True)\n",
       "    (act): ReLU()\n",
       "    (lstm_layer): LSTM(300, 300, num_layers=2, bidirectional=True)\n",
       "    (attention): Attention(\n",
       "      (Ws): Linear(in_features=600, out_features=50, bias=False)\n",
       "      (Wa): Linear(in_features=50, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (fc_in): Linear(in_features=2400, out_features=600, bias=True)\n",
       "  (act): ReLU()\n",
       "  (fc_out): Linear(in_features=600, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "data_module.set_fold(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "source": [
    "EPOCHS = 5\n",
    "for ep in range(EPOCHS):\n",
    "    train_loss = train(model, data_module, optimizer, device)\n",
    "    print(f\"\\tTraining Loss => epoch {ep}: {train_loss}\")\n",
    "    test_loss, test_acc, test_prec, test_recall, test_f1 = evaluate(model, data_module, device)\n",
    "    print(f\"\\t\\t Loss : {test_loss}, Accuracy: {test_acc}, Precsion: {test_prec}, Recall: {test_recall}, F1 Score:{test_f1}\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 151/151 [05:22<00:00,  2.13s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\tTraining Loss => epoch 0: 0.5480189297767665\n",
      "100%|██████████| 17/17 [00:12<00:00,  1.37it/s]\n",
      "  0%|          | 0/151 [00:00<?, ?it/s]\t\t Loss : 0.9107790414024802, Accuracy: 0.4528186321258545, Precsion: 0.45179595137253126, Recall: 0.45523759651081636, F1 Score:0.44123197038668266\n",
      "100%|██████████| 151/151 [05:21<00:00,  2.13s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\tTraining Loss => epoch 1: 0.325460375835564\n",
      "100%|██████████| 17/17 [00:12<00:00,  1.35it/s]\n",
      "  0%|          | 0/151 [00:00<?, ?it/s]\t\t Loss : 1.1092724449494307, Accuracy: 0.4522058963775635, Precsion: 0.4485660304172613, Recall: 0.45167919316867594, F1 Score:0.43739180629717855\n",
      "100%|██████████| 151/151 [05:22<00:00,  2.14s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\tTraining Loss => epoch 2: 0.19187185950330551\n",
      "100%|██████████| 17/17 [00:12<00:00,  1.38it/s]\n",
      "  0%|          | 0/151 [00:00<?, ?it/s]\t\t Loss : 1.2618366164319657, Accuracy: 0.46078431606292725, Precsion: 0.45863257111242944, Recall: 0.45305683106630107, F1 Score:0.4372600533327027\n",
      "100%|██████████| 151/151 [05:23<00:00,  2.14s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\tTraining Loss => epoch 3: 0.11469524150208527\n",
      "100%|██████████| 17/17 [00:12<00:00,  1.38it/s]\n",
      "  0%|          | 0/151 [00:00<?, ?it/s]\t\t Loss : 1.8572260281618904, Accuracy: 0.45526960492134094, Precsion: 0.45082948570507264, Recall: 0.45769787917620736, F1 Score:0.42754866018227655\n",
      "100%|██████████| 151/151 [05:21<00:00,  2.13s/it]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\tTraining Loss => epoch 4: 0.07047962143536139\n",
      "100%|██████████| 17/17 [00:12<00:00,  1.37it/s]\t\t Loss : 2.1262640953063965, Accuracy: 0.4534313678741455, Precsion: 0.4466345129257464, Recall: 0.4489255863675368, F1 Score:0.43898291283087093\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.36it/s][tensor(0.4375), tensor(0.4062), tensor(0.4375), tensor(0.7812), tensor(0.5000), tensor(0.2500), tensor(0.4062), tensor(0.5000), tensor(0.3438), tensor(0.4688), tensor(0.5625), tensor(0.3125), tensor(0.5625), tensor(0.5000), tensor(0.4062), tensor(0.4375), tensor(0.3750)]\n",
      "\t\t Loss : 2.115576751091901, Accuracy: 0.4522058963775635, Precsion: 0.4524940848350525, Recall: 0.4556811451911926, F1 Score:0.43799063563346863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_prec, test_recall, test_f1 = evaluate(model, data_module, device)\n",
    "print(f\"\\t\\t Loss : {test_loss}, Accuracy: {test_acc}, Precsion: {test_prec}, Recall: {test_recall}, F1 Score:{test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.metrics import Accuracy,F1,Precision,Recall\n",
    "\n",
    "def evaluate(model, data_module, device):\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    precision_values = []\n",
    "    recall_values = []\n",
    "    f1_values = []\n",
    "    acc_metric = Accuracy(num_classes=2)\n",
    "    prec_metric = Precision(num_classes=2)\n",
    "    recall_metric = Recall(num_classes=2)\n",
    "    F1_metric = F1(num_classes=2)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_module.test_dataloader()):\n",
    "            x0, x1, y = batch\n",
    "            model.zero_grad()\n",
    "            opt = model(x0.to(device), x1.to(device)).squeeze(1)\n",
    "            loss = F.cross_entropy(opt, y.to(device))\n",
    "            pred = F.softmax(opt)\n",
    "\n",
    "            loss_values.append(loss.cpu().item())\n",
    "\n",
    "            accuracy_values.append(\n",
    "                acc_metric(pred.argmax(1).cpu(),y.cpu())\n",
    "            )\n",
    "            precision_values.append(prec_metric(pred.argmax(1).cpu(),y.cpu()))\n",
    "            recall_values.append(recall_metric(pred.argmax(1).cpu(),y.cpu()))\n",
    "            f1_values.append(F1_metric(pred.argmax(1).cpu(),y.cpu()))\n",
    "\n",
    "    print(accuracy_values)\n",
    "    return (\n",
    "        np.mean(loss_values),\n",
    "        np.mean(accuracy_values),\n",
    "        np.mean(precision_values),\n",
    "        np.mean(recall_values),\n",
    "        np.mean(f1_values),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/17 [00:01<?, ?it/s]tensor([[-1.5441,  1.5205],\n",
      "        [ 4.6311, -4.4921],\n",
      "        [-0.7040,  0.6580],\n",
      "        [ 3.7006, -3.5853],\n",
      "        [ 0.1015, -0.1009],\n",
      "        [ 3.7428, -3.6935],\n",
      "        [ 4.3301, -4.2176],\n",
      "        [-1.3374,  1.3062],\n",
      "        [-0.5537,  0.5221],\n",
      "        [ 0.5367, -0.5602],\n",
      "        [-1.5412,  1.4964],\n",
      "        [ 0.0832, -0.1174],\n",
      "        [ 1.7061, -1.6620],\n",
      "        [-0.3722,  0.3431],\n",
      "        [ 4.5172, -4.3984],\n",
      "        [-0.2374,  0.2032],\n",
      "        [ 0.2591, -0.2440],\n",
      "        [ 1.7056, -1.6910],\n",
      "        [-0.2993,  0.2478],\n",
      "        [ 2.3985, -2.3388],\n",
      "        [ 0.1920, -0.2103],\n",
      "        [-0.8371,  0.8153],\n",
      "        [-1.3374,  1.3062],\n",
      "        [ 3.6410, -3.5456],\n",
      "        [-1.3374,  1.3062],\n",
      "        [-1.7551,  1.6844],\n",
      "        [ 1.0876, -1.0989],\n",
      "        [-0.9657,  0.9343],\n",
      "        [-0.5913,  0.5412],\n",
      "        [ 3.2502, -3.1724],\n",
      "        [ 1.2195, -1.2033],\n",
      "        [-1.2263,  1.1789]], device='cuda:0') tensor([[4.4589e-02, 9.5541e-01],\n",
      "        [9.9989e-01, 1.0909e-04],\n",
      "        [2.0391e-01, 7.9609e-01],\n",
      "        [9.9932e-01, 6.8472e-04],\n",
      "        [5.5044e-01, 4.4956e-01],\n",
      "        [9.9941e-01, 5.8915e-04],\n",
      "        [9.9981e-01, 1.9396e-04],\n",
      "        [6.6384e-02, 9.3362e-01],\n",
      "        [2.5432e-01, 7.4568e-01],\n",
      "        [7.4967e-01, 2.5033e-01],\n",
      "        [4.5755e-02, 9.5425e-01],\n",
      "        [5.4999e-01, 4.5001e-01],\n",
      "        [9.6669e-01, 3.3306e-02],\n",
      "        [3.2842e-01, 6.7158e-01],\n",
      "        [9.9987e-01, 1.3427e-04],\n",
      "        [3.9161e-01, 6.0839e-01],\n",
      "        [6.2319e-01, 3.7681e-01],\n",
      "        [9.6760e-01, 3.2404e-02],\n",
      "        [3.6654e-01, 6.3346e-01],\n",
      "        [9.9131e-01, 8.6863e-03],\n",
      "        [5.9924e-01, 4.0076e-01],\n",
      "        [1.6079e-01, 8.3921e-01],\n",
      "        [6.6384e-02, 9.3362e-01],\n",
      "        [9.9924e-01, 7.5606e-04],\n",
      "        [6.6384e-02, 9.3362e-01],\n",
      "        [3.1084e-02, 9.6892e-01],\n",
      "        [8.9903e-01, 1.0097e-01],\n",
      "        [1.3012e-01, 8.6988e-01],\n",
      "        [2.4369e-01, 7.5631e-01],\n",
      "        [9.9838e-01, 1.6219e-03],\n",
      "        [9.1855e-01, 8.1452e-02],\n",
      "        [8.2777e-02, 9.1722e-01]], device='cuda:0') tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in tqdm(data_module.test_dataloader()):\n",
    "        x0,x1,y = batch\n",
    "        opt = model(x0.to(device), x1.to(device)).squeeze(1)\n",
    "        loss = F.cross_entropy(opt, y.to(device))\n",
    "        pred = F.softmax(opt)\n",
    "        print(opt,pred,y)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import svn.remote\n",
    "\n",
    "r = svn.remote.RemoteClient('http://svn.dridan.com/sandpit/QA/trecdata/datacollection/')\n",
    "r.checkout('/content/DeepDocumentNovelty/trecdatasvn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}