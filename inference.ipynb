{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.nli_models import *\n",
    "from src.model.novelty_models import *\n",
    "from src.defaults import *\n",
    "from torchtext.data import Example \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html\n",
    "import random\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import IFrame\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def encode_text(text,field):\n",
    "    ex = Example.fromlist([text],[(\"text\",field)])\n",
    "    enc = field.process([ex.text])\n",
    "    return torch.tensor(enc)\n",
    "\n",
    "def load_novelty_model(_id):\n",
    "    # load model data \n",
    "    check_model(_id)\n",
    "    def load_model_data(_id):\n",
    "        model_path = os.path.join(\"./results/\", _id, \"model.pt\")\n",
    "        model_data = torch.load(model_path)\n",
    "        return model_data\n",
    "    field = load_field(_id)\n",
    "    model_data = load_model_data(_id)\n",
    "    encoder_id = model_data[\"options\"][\"load_nli\"]\n",
    "    check_model(encoder_id)\n",
    "\n",
    "    def load_encoder(enc_data):\n",
    "        if enc_data[\"options\"].get(\"attention_layer_param\", 0) == 0:\n",
    "            enc_data[\"options\"][\"use_glove\"] = False\n",
    "            model = bilstm_snli(enc_data[\"options\"])\n",
    "        elif enc_data[\"options\"].get(\"r\", 0) == 0:\n",
    "            enc_data[\"options\"][\"use_glove\"] = False\n",
    "            model = attn_bilstm_snli(enc_data[\"options\"])\n",
    "        else:\n",
    "            enc_data[\"options\"][\"use_glove\"] = False\n",
    "            model = struc_attn_snli(enc_data[\"options\"])\n",
    "        model.load_state_dict(enc_data[\"model_dict\"])\n",
    "        return model\n",
    "    \n",
    "    enc_data = load_encoder_data(encoder_id)\n",
    "    encoder = load_encoder(enc_data).encoder\n",
    "\n",
    "    model = HAN(model_data[\"options\"],encoder)\n",
    "    model.load_state_dict(model_data[\"model_dict\"])\n",
    "    return model,field\n",
    "\n",
    "def decode(inp,field):\n",
    "    if hasattr(field.nesting_field,\"vocab\"):\n",
    "        return [[field.nesting_field.vocab.itos[i] for i in sent] for sent in inp]\n",
    "    else:\n",
    "        tok = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        return [tok.convert_ids_to_tokens(i) for i in inp.tolist()]\n",
    "\n",
    "\n",
    "def attention_combined(inp,field,s_att,w_att=None):\n",
    "    tok_str = decode(inp,field)\n",
    "    assert len(tok_str) == s_att.shape[0]\n",
    "    assert len(tok_str) == w_att.shape[0]\n",
    "    assert len(tok_str[0]) == w_att.shape[1]\n",
    "    \n",
    "\n",
    "    opt = []\n",
    "    for sent in range(len(tok_str)):\n",
    "        sent_with_att = []\n",
    "        for word in range(len(tok_str[0])):\n",
    "            word_str = tok_str[sent][word]\n",
    "            if word_str not in [\"<pad>\",'[PAD]']:\n",
    "                sent_with_att.append((word_str,w_att[sent][word].item()))\n",
    "        if sent_with_att!=[]:\n",
    "            opt.append((sent_with_att,s_att[sent].item()))\n",
    "    return opt\n",
    "        \n",
    "\n",
    "\n",
    "def html_string(word,color,new_line = False):\n",
    "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
    "    colored_string = template.format(color, '&nbsp' + word + '&nbsp') + (\"<br>\" if new_line else \"\")\n",
    "    return colored_string\n",
    "\n",
    "\n",
    "def colorize(attention_list):\n",
    "    cmap_sent = matplotlib.cm.Blues\n",
    "    cmap_word = matplotlib.cm.Reds\n",
    "\n",
    "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
    "    colored_string = ''\n",
    "\n",
    "    for sent, sent_att in attention_list:\n",
    "        sent_color = matplotlib.colors.rgb2hex(cmap_sent(sent_att*1)[:3])\n",
    "        colored_string  += html_string('\\t---\\t ',sent_color)\n",
    "        for word,word_att in sent:\n",
    "            word_color = matplotlib.colors.rgb2hex(cmap_word(word_att)[:3])\n",
    "            colored_string += html_string(word,word_color)\n",
    "        colored_string += \"<br>\"\n",
    "    colored_string += \"<br><br><br><br>\"\n",
    "    return colored_string\n",
    "\n",
    "    seed_torch()\n",
    "\n",
    "def plot_attention(src,trg,model,field):\n",
    "    s_enc = encode_text(src,field)\n",
    "    t_enc = encode_text(trg,field)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        opt,s_att,t_att = model.forward_with_attn(s_enc,t_enc)\n",
    "        pred = F.softmax(opt)\n",
    "\n",
    "    src_att_map = attention_combined(s_enc[0],field,s_att[0][0,:,0],s_att[1][0])\n",
    "    trg_att_map = attention_combined(t_enc[0],field,t_att[0][0,:,0],t_att[1][0])\n",
    "\n",
    "    s_html = colorize(src_att_map)\n",
    "    t_html = colorize(trg_att_map)\n",
    "    with open('colorize.html', 'w') as f:\n",
    "        f.write(s_html+t_html)\n",
    "    print(pred)\n",
    "\n",
    "def disp_attention():\n",
    "    IFrame('./colorize.html',width=1200,height=400)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,field = load_novelty_model('NOV-1145')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"We also experimented with the document encoder to find if document level pretraining has any impact on the novelty detection performance. We train our document encoder described in on the Reuters dataset with an objective of 10 class classification. The reuters dataset aligns with the dataset we use for novelty detection, the Reuters dataset contains news articles which are to be classified into categories like Investment, Shipping, Crop, Oil and so on\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Identifing each of these classes requires the ability to extract features which tell which industry the news is related to. We hypothesise that this information is also essential while calculating the novelty of a document, since knowing if the target document is talking about the same thing or topic is also important. This can be seen as assisting the information filtering task. For this experiment we have 3 settings, we test the impact with and without pretraining for Reuters dataset and Reuters+NLI dataset combined. The settings used are listed below.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_hops': 10,\n",
       " 'attention_input': 400,\n",
       " 'attention_layer_param': 200,\n",
       " 'dataset': 'dlnd',\n",
       " 'device': 'cuda',\n",
       " 'dropout': 0.3,\n",
       " 'encoder_dim': 400,\n",
       " 'folds': False,\n",
       " 'freeze_encoder': False,\n",
       " 'hidden_size': 400,\n",
       " 'labeled': -1,\n",
       " 'load_han': 'None',\n",
       " 'load_nli': 'NLI-93',\n",
       " 'max_num_sent': 50,\n",
       " 'num_layers': 1,\n",
       " 'reset_enc': False,\n",
       " 'results_dir': 'results',\n",
       " 'secondary_dataset': 'None',\n",
       " 'seed': -1,\n",
       " 'sent_tokenizer': 'spacy'}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ",\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192],\n         [ 0.0004,  0.0159,  0.0234,  0.0238,  0.0238, -0.0176, -0.0311,\n           0.0259,  0.0294, -0.0192]]])\ntensor([[[0.0197, 0.0207, 0.0205, 0.0198, 0.0202, 0.0203, 0.0200, 0.0196,\n          0.0211, 0.0210],\n         [0.0195, 0.0247, 0.0228, 0.0196, 0.0216, 0.0220, 0.0197, 0.0188,\n          0.0271, 0.0254],\n         [0.0197, 0.0202, 0.0201, 0.0198, 0.0200, 0.0202, 0.0199, 0.0196,\n          0.0204, 0.0204],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199],\n         [0.0200, 0.0199, 0.0199, 0.0200, 0.0200, 0.0199, 0.0200, 0.0200,\n          0.0198, 0.0199]]])\ntensor([[[-4.8375e-02,  2.7204e-01,  1.7572e-01, -9.6080e-03,  1.2864e-01,\n           9.8627e-02, -8.8428e-02, -5.5644e-02,  4.3040e-01,  3.0636e-01],\n         [-1.0128e-02,  3.1137e-02,  3.3928e-02,  2.1539e-02,  2.6492e-02,\n          -2.0934e-02, -3.5985e-02,  1.2975e-02,  5.1267e-02,  3.4991e-03],\n         [-3.0395e-02,  6.6464e-02,  5.8241e-02,  1.1222e-02,  3.8939e-02,\n          -3.5917e-03, -4.2168e-02, -7.2842e-03,  1.1187e-01,  4.8106e-02],\n         [ 3.9631e-02,  5.1246e-02,  2.9445e-02,  5.6585e-03,  1.8085e-02,\n           3.0951e-02,  2.6551e-02,  7.1633e-03,  3.5194e-02,  2.2708e-02],\n         [-3.6455e-02,  7.4467e-02,  6.1043e-02,  2.6142e-03,  3.7105e-02,\n           5.1173e-03, -4.5334e-02, -2.1965e-02,  1.3087e-01,  6.3269e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02],\n         [ 3.7546e-04,  1.5920e-02,  2.3400e-02,  2.3841e-02,  2.3759e-02,\n          -1.7613e-02, -3.1099e-02,  2.5882e-02,  2.9379e-02, -1.9186e-02]]])\ntensor([[[0.0191, 0.0256, 0.0232, 0.0194, 0.0222, 0.0224, 0.0189, 0.0185,\n          0.0294, 0.0274],\n         [0.0198, 0.0201, 0.0201, 0.0200, 0.0200, 0.0199, 0.0199, 0.0198,\n          0.0202, 0.0202],\n         [0.0194, 0.0208, 0.0206, 0.0198, 0.0203, 0.0202, 0.0198, 0.0194,\n          0.0214, 0.0211],\n         [0.0208, 0.0205, 0.0200, 0.0197, 0.0198, 0.0209, 0.0212, 0.0197,\n          0.0198, 0.0206],\n         [0.0193, 0.0210, 0.0207, 0.0196, 0.0202, 0.0204, 0.0197, 0.0191,\n          0.0218, 0.0215],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198],\n         [0.0200, 0.0198, 0.0199, 0.0200, 0.0199, 0.0199, 0.0200, 0.0201,\n          0.0197, 0.0198]]])\n0.019652923569083214\n0.019505050033330917\n0.01970778778195381\n0.019080596044659615\n0.019824516028165817\n0.019426783546805382\n0.02083592675626278\n0.019309407100081444\ntensor([[0.8894, 0.1106]])\n"
     ]
    }
   ],
   "source": [
    "a = plot_attention(source,target,model,field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f49fed926d0>"
      ],
      "text/html": "\n        <iframe\n            width=\"2200\"\n            height=\"1000\"\n            src=\"./colorize.html\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "IFrame('./colorize.html',width=2200,height=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.data/dlnd/TAP-DLND-1.0_LREC2018_modified/dlnd.jsonl','r') as f:\n",
    "    items = f.readlines()\n",
    "data = [json.loads(i) for i in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction:\n",
      "tensor([[1., 0.]])\n",
      "Actual:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Novel'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "example = data[886]\n",
    "print(\"Prediction:\")\n",
    "plot_attention(example[\"source\"],example[\"target_text\"],model,field)\n",
    "print(\"Actual:\")\n",
    "example[\"DLA\"]"
   ]
  },
  {
   "source": [
    "IFrame('./colorize.html',width=2200,height=2000)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f219d8d5110>"
      ],
      "text/html": "\n        <iframe\n            width=\"2200\"\n            height=\"2000\"\n            src=\"./colorize.html\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "
     },
     "metadata": {},
     "execution_count": 30
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "284\n"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "for i in data:\n",
    "    lens.append(len(i['source']))\n",
    "print(lens.index(min(lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [(i,lens[i]) for i in range(len(lens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "\n",
    "from tqdm import tqdm\n",
    "def predict(data,model,field):\n",
    "    wrong_id = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        src = data[i]['source']\n",
    "        trg = data[i]['target_text']\n",
    "        true = data[i]['DLA']\n",
    "        s_enc = encode_text(src,field)\n",
    "        t_enc = encode_text(trg,field)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            opt,s_att,t_att = model.forward_with_attn(s_enc.cuda(),t_enc.cuda())\n",
    "            pred = F.softmax(opt)[0][1].item()\n",
    "        if pred > 0.5:\n",
    "            pred = \"Novel\"\n",
    "        else:\n",
    "            pred = \"Non-Novel\"\n",
    "        if pred!=true:\n",
    "            wrong_id.append(i)\n",
    "    return wrong_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5435/5435 [02:52<00:00, 31.56it/s]\n"
     ]
    }
   ],
   "source": [
    "wrong_id = predict(data,model,field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "HAN(\n",
       "  (encoder): HAN_DOC(\n",
       "    (encoder): Attn_Encoder(\n",
       "      (embedding): Embedding(33934, 300, padding_idx=1)\n",
       "      (translate): Linear(in_features=300, out_features=400, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (lstm_layer): LSTM(400, 400, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "      (attention): Attention(\n",
       "        (Ws): Linear(in_features=800, out_features=200, bias=False)\n",
       "        (Wa): Linear(in_features=200, out_features=1, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (translate): Linear(in_features=800, out_features=400, bias=True)\n",
       "    (act): ReLU()\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (lstm_layer): LSTM(400, 400, bidirectional=True)\n",
       "    (attention): StrucSelfAttention(\n",
       "      (ut_dense): Linear(in_features=800, out_features=200, bias=False)\n",
       "      (et_dense): Linear(in_features=200, out_features=10, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (act): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=32000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(292, 1737)\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for i in sorted(lens,key = lambda x:x[1]): \n",
    "    c+=1\n",
    "    if i[0] in wrong_id:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[69,\n",
       " 83,\n",
       " 112,\n",
       " 131,\n",
       " 148,\n",
       " 188,\n",
       " 218,\n",
       " 265,\n",
       " 290,\n",
       " 294,\n",
       " 296,\n",
       " 316,\n",
       " 378,\n",
       " 380,\n",
       " 381,\n",
       " 410,\n",
       " 419,\n",
       " 440,\n",
       " 471,\n",
       " 473,\n",
       " 476,\n",
       " 517,\n",
       " 525,\n",
       " 535,\n",
       " 537,\n",
       " 549,\n",
       " 691,\n",
       " 693,\n",
       " 707,\n",
       " 717,\n",
       " 741,\n",
       " 755,\n",
       " 756,\n",
       " 767,\n",
       " 773,\n",
       " 791,\n",
       " 805,\n",
       " 816,\n",
       " 833,\n",
       " 839,\n",
       " 841,\n",
       " 842,\n",
       " 851,\n",
       " 853,\n",
       " 872,\n",
       " 881,\n",
       " 882,\n",
       " 886,\n",
       " 890,\n",
       " 939,\n",
       " 1024,\n",
       " 1059,\n",
       " 1070,\n",
       " 1103,\n",
       " 1123,\n",
       " 1138,\n",
       " 1141,\n",
       " 1180,\n",
       " 1203,\n",
       " 1217,\n",
       " 1233,\n",
       " 1263,\n",
       " 1264,\n",
       " 1268,\n",
       " 1280,\n",
       " 1286,\n",
       " 1300,\n",
       " 1306,\n",
       " 1313,\n",
       " 1332,\n",
       " 1338,\n",
       " 1362,\n",
       " 1416,\n",
       " 1628,\n",
       " 1637,\n",
       " 1646,\n",
       " 1647,\n",
       " 1648,\n",
       " 1658,\n",
       " 1696,\n",
       " 1697,\n",
       " 1701,\n",
       " 1714,\n",
       " 1746,\n",
       " 1753,\n",
       " 1754,\n",
       " 1756,\n",
       " 1757,\n",
       " 1759,\n",
       " 1760,\n",
       " 1761,\n",
       " 1769,\n",
       " 1771,\n",
       " 1774,\n",
       " 1778,\n",
       " 1783,\n",
       " 1795,\n",
       " 1805,\n",
       " 1830,\n",
       " 1838,\n",
       " 1839,\n",
       " 1872,\n",
       " 1902,\n",
       " 1910,\n",
       " 1964,\n",
       " 1988,\n",
       " 2015,\n",
       " 2035,\n",
       " 2052,\n",
       " 2057,\n",
       " 2058,\n",
       " 2063,\n",
       " 2070,\n",
       " 2084,\n",
       " 2088,\n",
       " 2120,\n",
       " 2123,\n",
       " 2126,\n",
       " 2136,\n",
       " 2158,\n",
       " 2165,\n",
       " 2174,\n",
       " 2176,\n",
       " 2188,\n",
       " 2255,\n",
       " 2274,\n",
       " 2276,\n",
       " 2281,\n",
       " 2287,\n",
       " 2289,\n",
       " 2296,\n",
       " 2304,\n",
       " 2305,\n",
       " 2314,\n",
       " 2323,\n",
       " 2324,\n",
       " 2326,\n",
       " 2327,\n",
       " 2341,\n",
       " 2354,\n",
       " 2355,\n",
       " 2373,\n",
       " 2398,\n",
       " 2427,\n",
       " 2435,\n",
       " 2439,\n",
       " 2447,\n",
       " 2533,\n",
       " 2565,\n",
       " 2586,\n",
       " 2599,\n",
       " 2645,\n",
       " 2652,\n",
       " 2654,\n",
       " 2660,\n",
       " 2666,\n",
       " 2691,\n",
       " 2706,\n",
       " 2712,\n",
       " 2720,\n",
       " 2721,\n",
       " 2754,\n",
       " 2761,\n",
       " 2770,\n",
       " 2774,\n",
       " 2803,\n",
       " 2823,\n",
       " 2865,\n",
       " 2873,\n",
       " 2882,\n",
       " 2909,\n",
       " 2925,\n",
       " 2936,\n",
       " 2942,\n",
       " 2957,\n",
       " 3023,\n",
       " 3027,\n",
       " 3055,\n",
       " 3074,\n",
       " 3106,\n",
       " 3138,\n",
       " 3155,\n",
       " 3157,\n",
       " 3158,\n",
       " 3160,\n",
       " 3162,\n",
       " 3163,\n",
       " 3164,\n",
       " 3165,\n",
       " 3168,\n",
       " 3169,\n",
       " 3171,\n",
       " 3172,\n",
       " 3175,\n",
       " 3181,\n",
       " 3182,\n",
       " 3183,\n",
       " 3184,\n",
       " 3191,\n",
       " 3192,\n",
       " 3194,\n",
       " 3195,\n",
       " 3196,\n",
       " 3198,\n",
       " 3199,\n",
       " 3201,\n",
       " 3202,\n",
       " 3203,\n",
       " 3204,\n",
       " 3206,\n",
       " 3207,\n",
       " 3208,\n",
       " 3209,\n",
       " 3210,\n",
       " 3213,\n",
       " 3214,\n",
       " 3216,\n",
       " 3217,\n",
       " 3221,\n",
       " 3223,\n",
       " 3227,\n",
       " 3228,\n",
       " 3229,\n",
       " 3230,\n",
       " 3231,\n",
       " 3232,\n",
       " 3234,\n",
       " 3235,\n",
       " 3238,\n",
       " 3240,\n",
       " 3242,\n",
       " 3246,\n",
       " 3247,\n",
       " 3248,\n",
       " 3251,\n",
       " 3253,\n",
       " 3254,\n",
       " 3255,\n",
       " 3256,\n",
       " 3258,\n",
       " 3261,\n",
       " 3263,\n",
       " 3264,\n",
       " 3265,\n",
       " 3266,\n",
       " 3268,\n",
       " 3274,\n",
       " 3276,\n",
       " 3296,\n",
       " 3298,\n",
       " 3307,\n",
       " 3312,\n",
       " 3417,\n",
       " 3434,\n",
       " 3475,\n",
       " 3505,\n",
       " 3510,\n",
       " 3549,\n",
       " 3575,\n",
       " 3617,\n",
       " 3638,\n",
       " 3671,\n",
       " 3709,\n",
       " 3726,\n",
       " 3776,\n",
       " 3804,\n",
       " 3821,\n",
       " 3835,\n",
       " 3844,\n",
       " 3929,\n",
       " 3945,\n",
       " 4077,\n",
       " 4081,\n",
       " 4099,\n",
       " 4107,\n",
       " 4108,\n",
       " 4110,\n",
       " 4111,\n",
       " 4112,\n",
       " 4113,\n",
       " 4114,\n",
       " 4115,\n",
       " 4117,\n",
       " 4129,\n",
       " 4141,\n",
       " 4142,\n",
       " 4143,\n",
       " 4167,\n",
       " 4170,\n",
       " 4180,\n",
       " 4185,\n",
       " 4268,\n",
       " 4270,\n",
       " 4292,\n",
       " 4295,\n",
       " 4315,\n",
       " 4425,\n",
       " 4532,\n",
       " 4591,\n",
       " 4622,\n",
       " 4642,\n",
       " 4684,\n",
       " 4750,\n",
       " 4759,\n",
       " 4780,\n",
       " 4796,\n",
       " 4802,\n",
       " 4881,\n",
       " 4905,\n",
       " 4937,\n",
       " 4940,\n",
       " 4942,\n",
       " 4963,\n",
       " 4964,\n",
       " 4967,\n",
       " 4977,\n",
       " 4979,\n",
       " 4983,\n",
       " 4996,\n",
       " 4999,\n",
       " 5012,\n",
       " 5013,\n",
       " 5022,\n",
       " 5023,\n",
       " 5035,\n",
       " 5038,\n",
       " 5096,\n",
       " 5115,\n",
       " 5132,\n",
       " 5149,\n",
       " 5158,\n",
       " 5162,\n",
       " 5170,\n",
       " 5179,\n",
       " 5223,\n",
       " 5324,\n",
       " 5352,\n",
       " 5397,\n",
       " 5398,\n",
       " 5420]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "wrong_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}