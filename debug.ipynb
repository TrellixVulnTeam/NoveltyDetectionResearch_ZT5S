{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN_conf:\n",
    "    num_sent = 100\n",
    "    sent_len = 100\n",
    "    encoder_dim = 400\n",
    "    hidden_size = 400\n",
    "    activation = 'relu'\n",
    "    dropout = 0.3\n",
    "\n",
    "    def __init__(self, num_sent, encoder, **kwargs):\n",
    "        self.num_sent = num_sent\n",
    "        self.encoder = encoder\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class DAN(nn.Module):\n",
    "    def __init__(self,conf):\n",
    "        super(DAN,self).__init__()\n",
    "        self.conf = conf\n",
    "        self.sent_len = conf.sent_len\n",
    "        self.num_sent = conf.num_sent\n",
    "        self.encoder = conf.encoder\n",
    "        del self.conf.encoder\n",
    "        self.translate = nn.Linear(2 * self.conf.encoder_dim, self.conf.hidden_size)\n",
    "        self.template = nn.Parameter(torch.zeros((1)), requires_grad=True)\n",
    "        if self.conf.activation.lower() == \"relu\".lower():\n",
    "            self.act = nn.ReLU()\n",
    "        elif self.conf.activation.lower() == \"tanh\".lower():\n",
    "            self.act = nn.Tanh()\n",
    "        elif self.conf.activation.lower() == \"leakyrelu\".lower():\n",
    "            self.act = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(conf.dropout)\n",
    "\n",
    "        self.mlp_f = nn.Linear(self.conf.hidden_size, self.conf.hidden_size)\n",
    "        self.mlp_g = nn.Linear(2*self.conf.hidden_size, self.conf.hidden_size)\n",
    "        self.mlp_h = nn.Linear(2*self.conf.hidden_size, self.conf.hidden_size)\n",
    "        self.linear = nn.Linear(self.conf.hidden_size,2)\n",
    "\n",
    "    def encode_sent(self,inp):\n",
    "        batch_size,_,_ = inp.shape\n",
    "        x = inp.view(-1,self.sent_len)\n",
    "\n",
    "        x_padded_idx = x.sum(dim=1) != 0    \n",
    "        x_enc = []\n",
    "        for sub_batch in x[x_padded_idx].split(64):\n",
    "            x_enc.append(self.encoder(sub_batch)[0])\n",
    "        x_enc = torch.cat(x_enc, dim=0)\n",
    "\n",
    "        x_enc_t = torch.zeros((batch_size * self.num_sent, x_enc.size(1))).to(\n",
    "            self.template.device\n",
    "        )\n",
    "\n",
    "        x_enc_t[x_padded_idx] = x_enc\n",
    "        x_enc_t = x_enc_t.view(batch_size, self.num_sent, -1)\n",
    "    \n",
    "        embedded = self.dropout(self.translate(x_enc_t))\n",
    "        embedded = self.act(embedded)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        return embedded\n",
    "\n",
    "\n",
    "    def forward(self,x0,x1):\n",
    "        x0_enc = self.encode_sent(x0).permute(1,0,2)\n",
    "        x1_enc = self.encode_sent(x1).permute(1,0,2)\n",
    "\n",
    "        f1 = self.act(self.dropout(self.mlp_f(x0_enc)))\n",
    "        f2 = self.act(self.dropout(self.mlp_f(x1_enc)))\n",
    "\n",
    "        score1 = torch.bmm(f1, torch.transpose(f2, 1, 2))\n",
    "        prob1 = F.softmax(score1.view(-1, self.num_sent)).view(-1, self.num_sent, self.num_sent)\n",
    "\n",
    "        score2 = torch.transpose(score1.contiguous(), 1, 2)\n",
    "        score2 = score2.contiguous()\n",
    "\n",
    "        prob2 = F.softmax(score2.view(-1, self.num_sent)).view(-1, self.num_sent, self.num_sent)\n",
    "\n",
    "        sent1_combine = torch.cat((x0_enc, torch.bmm(prob1, x1_enc)), 2)\n",
    "        sent2_combine = torch.cat((x1_enc, torch.bmm(prob2, x0_enc)), 2)\n",
    "\n",
    "        \n",
    "\n",
    "        g1 = self.act(self.dropout(self.mlp_g(sent1_combine)))\n",
    "        g2 = self.act(self.dropout(self.mlp_g(sent2_combine)))\n",
    "\n",
    "        sent1_output = torch.sum(g1, 1)  \n",
    "        sent1_output = torch.squeeze(sent1_output, 1)\n",
    "    \n",
    "        sent2_output = torch.sum(g2, 1)  \n",
    "        sent2_output = torch.squeeze(sent2_output, 1)\n",
    "\n",
    "\n",
    "        input_combine = torch.cat((sent1_output * sent2_output, torch.abs(sent1_output - sent2_output)), 1)\n",
    "        \n",
    "        h = self.act(self.dropout(self.mlp_h(input_combine)))\n",
    "        opt = self.linear(h)\n",
    "        return opt\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_models import load_bilstm_encoder, load_attn_encoder\n",
    "from utils.helpers import seed_torch\n",
    "\n",
    "encoder, Lang = load_attn_encoder(\"SNLI-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = DAN_conf(20, encoder)\n",
    "model = DAN(model_conf)\n",
    "\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0,10000,[32,20,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([32, 20, 20]) torch.Size([32, 20, 20])\ntorch.Size([32, 20, 800]) torch.Size([32, 20, 800])\ntorch.Size([32, 20, 400]) torch.Size([32, 20, 400])\ntorch.Size([32, 400]) torch.Size([32, 400])\n"
     ]
    }
   ],
   "source": [
    "opt = model(x.cuda(),x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "print(opt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(-1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([640, 100])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded_idx = x.sum(dim=1) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([640])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "x_padded_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([640, 100])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "x[x_padded_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigjson\n",
    "with open(\"dataset/yelp/yelp_academic_dataset_review.json\", 'rb') as f:\n",
    "    data = bigjson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def json_reader(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_reader(\"dataset/yelp/yelp_academic_dataset_review.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "8021122it [01:13, 108871.54it/s]                             \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "data_json = {}\n",
    "count = 0\n",
    "for i in tqdm(data,total=8021122):\n",
    "    data_json[count] = {\"text\":i[\"text\"],\"label\":i[\"stars\"]}\n",
    "    count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i[\"label\"] for i in data_json.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def create_json():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    wsj = \"./dataset/trec/wsj\"\n",
    "    ap = \"./dataset/trec/ap\"\n",
    "    wsj_files = glob.glob(wsj + \"/*\")\n",
    "    ap_files = glob.glob(ap + \"/*\")\n",
    "\n",
    "    docs_json = {}\n",
    "    errors = 0\n",
    "    for wsj_file in wsj_files:\n",
    "        with open(wsj_file, \"r\") as f:\n",
    "            txt = f.read()\n",
    "        docs = [\n",
    "            i.split(\"<DOC>\")[1]\n",
    "            for i in filter(lambda x: len(x) > 10, txt.split(\"</DOC>\"))\n",
    "        ]\n",
    "\n",
    "        for doc in docs:\n",
    "            try:\n",
    "                id = doc.split(\"<DOCNO>\")[1].split(\"</DOCNO>\")[0]\n",
    "                text = doc.split(\"<TEXT>\")[1].split(\"</TEXT>\")[0]\n",
    "                docs_json[id] = text\n",
    "            except:\n",
    "                errors += 1\n",
    "\n",
    "    for ap_file in ap_files:\n",
    "        with open(ap_file, \"r\", encoding=\"latin-1\") as f:\n",
    "            txt = f.read()\n",
    "        docs = [\n",
    "            i.split(\"<DOC>\")[1]\n",
    "            for i in filter(lambda x: len(x) > 10, txt.split(\"</DOC>\"))\n",
    "        ]\n",
    "\n",
    "        for doc in docs:\n",
    "            try:\n",
    "                id = doc.split(\"<DOCNO>\")[1].split(\"</DOCNO>\")[0]\n",
    "                text = doc.split(\"<TEXT>\")[1].split(\"</TEXT>\")[0]\n",
    "                docs_json[id] = text\n",
    "            except:\n",
    "                errors += 1\n",
    "    print(\"Reading APWSJ dataset, Errors : \", errors)\n",
    "\n",
    "    docs_json = {k.strip(): v.strip() for k, v in docs_json.items()}\n",
    "\n",
    "    topic_to_doc_file = \"./dataset/apwsj/NoveltyData/apwsj.qrels\"\n",
    "    with open(topic_to_doc_file, \"r\") as f:\n",
    "        topic_to_doc = f.read()\n",
    "    topic_doc = [\n",
    "        (i.split(\" 0 \")[1][:-2], i.split(\" 0 \")[0]) for i in topic_to_doc.split(\"\\n\")\n",
    "    ]\n",
    "    topics = \"q101, q102, q103, q104, q105, q106, q107, q108, q109, q111, q112, q113, q114, q115, q116, q117, q118, q119, q120, q121, q123, q124, q125, q127, q128, q129, q132, q135, q136, q137, q138, q139, q141\"\n",
    "    topic_list = topics.split(\", \")\n",
    "    filterd_docid = [(k, v) for k, v in topic_doc if v in topic_list]\n",
    "\n",
    "    def crawl(red_dict, doc, crawled):\n",
    "        ans = []\n",
    "        for cdoc in red_dict[doc]:\n",
    "            ans.append(cdoc)\n",
    "            if crawled[cdoc] == 0:\n",
    "                try:\n",
    "                    red_dict[cdoc] = crawl(red_dict, cdoc, crawled)\n",
    "                    crawled[cdoc] = 1\n",
    "                    ans += red_dict[cdoc]\n",
    "                except:\n",
    "                    crawled[cdoc] = 1\n",
    "        return ans\n",
    "\n",
    "    wf = \"./dataset/apwsj/redundancy_list_without_partially_redundant.txt\"\n",
    "    topics_allowed = \"q101, q102, q103, q104, q105, q106, q107, q108, q109, q111, q112, q113, q114, q115, q116, q117, q118, q119, q120, q121, q123, q124, q125, q127, q128, q129, q132, q135, q136, q137, q138, q139, q141\"\n",
    "    topics_allowed = topics_allowed.split(\", \")\n",
    "    red_dict = dict()\n",
    "    allow_partially_redundant = 1\n",
    "    for line in open(\"./dataset/apwsj/NoveltyData/redundancy.apwsj.result\", \"r\"):\n",
    "        tokens = line.split()\n",
    "        if tokens[2] == \"?\":\n",
    "            if allow_partially_redundant == 1:\n",
    "                red_dict[tokens[0] + \"/\" + tokens[1]] = [\n",
    "                    tokens[0] + \"/\" + i for i in tokens[3:]\n",
    "                ]\n",
    "        else:\n",
    "            red_dict[tokens[0] + \"/\" + tokens[1]] = [\n",
    "                tokens[0] + \"/\" + i for i in tokens[2:]\n",
    "            ]\n",
    "    crawled = defaultdict(int)\n",
    "    for doc in red_dict:\n",
    "        if crawled[doc] == 0:\n",
    "            red_dict[doc] = crawl(red_dict, doc, crawled)\n",
    "            crawled[doc] = 1\n",
    "    with open(wf, \"w\") as f:\n",
    "        for doc in red_dict:\n",
    "            if doc.split(\"/\")[0] in topics_allowed:\n",
    "                f.write(\n",
    "                    \" \".join(doc.split(\"/\") + [i.split(\"/\")[1] for i in red_dict[doc]])\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "\n",
    "    write_file = \"./dataset/apwsj/novel_list_without_partially_redundant.txt\"\n",
    "    topics = topic_list\n",
    "    doc_topic_dict = defaultdict(list)\n",
    "\n",
    "    for i in topic_doc:\n",
    "        doc_topic_dict[i[0]].append(i[1])\n",
    "    docs_sorted = (\n",
    "        open(\"./dataset/apwsj/NoveltyData/apwsj88-90.rel.docno.sorted\", \"r\")\n",
    "        .read()\n",
    "        .splitlines()\n",
    "    )\n",
    "    sorted_doc_topic_dict = defaultdict(list)\n",
    "    for doc in docs_sorted:\n",
    "        if len(doc_topic_dict[doc]) > 0:\n",
    "            for t in doc_topic_dict[doc]:\n",
    "                sorted_doc_topic_dict[t].append(doc)\n",
    "    redundant_dict = defaultdict(lambda: defaultdict(int))\n",
    "    for line in open(\n",
    "        \"./dataset/apwsj/redundancy_list_without_partially_redundant.txt\", \"r\"\n",
    "    ):\n",
    "        tokens = line.split()\n",
    "        redundant_dict[tokens[0]][tokens[1]] = 1\n",
    "    novel_list = []\n",
    "    for topic in topics:\n",
    "        if topic in topics_allowed:\n",
    "            for i in range(len(sorted_doc_topic_dict[topic])):\n",
    "                if redundant_dict[topic][sorted_doc_topic_dict[topic][i]] != 1:\n",
    "                    if i > 0:\n",
    "                        # take at most 5 latest docs in case of novel\n",
    "                        novel_list.append(\n",
    "                            \" \".join(\n",
    "                                [topic, sorted_doc_topic_dict[topic][i]]\n",
    "                                + sorted_doc_topic_dict[topic][max(0, i - 5) : i]\n",
    "                            )\n",
    "                        )\n",
    "    with open(write_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(novel_list))\n",
    "\n",
    "    # Novel cases\n",
    "    novel_docs = \"./dataset/apwsj/novel_list_without_partially_redundant.txt\"\n",
    "    with open(novel_docs, \"r\") as f:\n",
    "        novel_doc_list = [i.split() for i in f.read().split(\"\\n\")]\n",
    "    # Redundant cases\n",
    "    red_docs = \"./dataset/apwsj/redundancy_list_without_partially_redundant.txt\"\n",
    "    with open(red_docs, \"r\") as f:\n",
    "        red_doc_list = [i.split() for i in f.read().split(\"\\n\")]\n",
    "    red_doc_list = filter(lambda x: len(x) > 0, red_doc_list)\n",
    "    novel_doc_list = filter(lambda x: len(x) > 0, novel_doc_list)\n",
    "\n",
    "    dataset = []\n",
    "    s_not_found = 0\n",
    "    t_not_found = 0\n",
    "    for i in novel_doc_list:\n",
    "        target_id = i[1]\n",
    "        source_ids = i[2:]\n",
    "        if target_id in docs_json.keys():\n",
    "            data_inst = {}\n",
    "            data_inst[\"target\"] = docs_json[target_id]\n",
    "            data_inst[\"source\"] = \"\"\n",
    "            for source_id in source_ids:\n",
    "                if source_id in docs_json.keys():\n",
    "                    data_inst[\"source\"] += docs_json[source_id] + \". \\n\"\n",
    "            data_inst[\"label\"] = 1\n",
    "        else:\n",
    "            print(target_id)\n",
    "        if data_inst[\"source\"] != \"\":\n",
    "            dataset.append(data_inst)\n",
    "\n",
    "    for i in red_doc_list:\n",
    "        target_id = i[1]\n",
    "        source_ids = i[2:]\n",
    "        if target_id in docs_json.keys():\n",
    "            data_inst = {}\n",
    "            data_inst[\"target\"] = docs_json[target_id]\n",
    "            data_inst[\"source\"] = \"\"\n",
    "            for source_id in source_ids:\n",
    "                if source_id in docs_json.keys():\n",
    "                    data_inst[\"source\"] += docs_json[source_id] + \". \\n\"\n",
    "            data_inst[\"label\"] = 0\n",
    "        else:\n",
    "            print(target_id)\n",
    "        if data_inst[\"source\"] != \"\":\n",
    "            dataset.append(data_inst)\n",
    "\n",
    "    dataset_json = {}\n",
    "    for i in range(len(dataset)):\n",
    "        dataset_json[i] = dataset[i]\n",
    "\n",
    "    dataset_path = \"./dataset/apwsj/apwsj_dataset.json\"\n",
    "    with open(dataset_path, \"w\") as f:\n",
    "        json.dump(dataset_json, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('need_these.txt','r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = data.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}