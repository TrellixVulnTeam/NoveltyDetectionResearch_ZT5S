{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigjson\n",
    "with open(\"dataset/yelp/yelp_academic_dataset_review.json\", 'rb') as f:\n",
    "    data = bigjson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def json_reader(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_reader(\"dataset/yelp/yelp_academic_dataset_review.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "8021122it [01:13, 108871.54it/s]                             \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "data_json = {}\n",
    "count = 0\n",
    "for i in tqdm(data,total=8021122):\n",
    "    data_json[count] = {\"text\":i[\"text\"],\"label\":i[\"stars\"]}\n",
    "    count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i[\"label\"] for i in data_json.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def create_json():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    wsj = \"./dataset/trec/wsj\"\n",
    "    ap = \"./dataset/trec/ap\"\n",
    "    wsj_files = glob.glob(wsj + \"/*\")\n",
    "    ap_files = glob.glob(ap + \"/*\")\n",
    "\n",
    "    docs_json = {}\n",
    "    errors = 0\n",
    "    for wsj_file in wsj_files:\n",
    "        with open(wsj_file, \"r\") as f:\n",
    "            txt = f.read()\n",
    "        docs = [\n",
    "            i.split(\"<DOC>\")[1]\n",
    "            for i in filter(lambda x: len(x) > 10, txt.split(\"</DOC>\"))\n",
    "        ]\n",
    "\n",
    "        for doc in docs:\n",
    "            try:\n",
    "                id = doc.split(\"<DOCNO>\")[1].split(\"</DOCNO>\")[0]\n",
    "                text = doc.split(\"<TEXT>\")[1].split(\"</TEXT>\")[0]\n",
    "                docs_json[id] = text\n",
    "            except:\n",
    "                errors += 1\n",
    "\n",
    "    for ap_file in ap_files:\n",
    "        with open(ap_file, \"r\", encoding=\"latin-1\") as f:\n",
    "            txt = f.read()\n",
    "        docs = [\n",
    "            i.split(\"<DOC>\")[1]\n",
    "            for i in filter(lambda x: len(x) > 10, txt.split(\"</DOC>\"))\n",
    "        ]\n",
    "\n",
    "        for doc in docs:\n",
    "            try:\n",
    "                id = doc.split(\"<DOCNO>\")[1].split(\"</DOCNO>\")[0]\n",
    "                text = doc.split(\"<TEXT>\")[1].split(\"</TEXT>\")[0]\n",
    "                docs_json[id] = text\n",
    "            except:\n",
    "                errors += 1\n",
    "    print(\"Reading APWSJ dataset, Errors : \", errors)\n",
    "\n",
    "    docs_json = {k.strip(): v.strip() for k, v in docs_json.items()}\n",
    "\n",
    "    topic_to_doc_file = \"./dataset/apwsj/NoveltyData/apwsj.qrels\"\n",
    "    with open(topic_to_doc_file, \"r\") as f:\n",
    "        topic_to_doc = f.read()\n",
    "    topic_doc = [\n",
    "        (i.split(\" 0 \")[1][:-2], i.split(\" 0 \")[0]) for i in topic_to_doc.split(\"\\n\")\n",
    "    ]\n",
    "    topics = \"q101, q102, q103, q104, q105, q106, q107, q108, q109, q111, q112, q113, q114, q115, q116, q117, q118, q119, q120, q121, q123, q124, q125, q127, q128, q129, q132, q135, q136, q137, q138, q139, q141\"\n",
    "    topic_list = topics.split(\", \")\n",
    "    filterd_docid = [(k, v) for k, v in topic_doc if v in topic_list]\n",
    "\n",
    "    def crawl(red_dict, doc, crawled):\n",
    "        ans = []\n",
    "        for cdoc in red_dict[doc]:\n",
    "            ans.append(cdoc)\n",
    "            if crawled[cdoc] == 0:\n",
    "                try:\n",
    "                    red_dict[cdoc] = crawl(red_dict, cdoc, crawled)\n",
    "                    crawled[cdoc] = 1\n",
    "                    ans += red_dict[cdoc]\n",
    "                except:\n",
    "                    crawled[cdoc] = 1\n",
    "        return ans\n",
    "\n",
    "    wf = \"./dataset/apwsj/redundancy_list_without_partially_redundant.txt\"\n",
    "    topics_allowed = \"q101, q102, q103, q104, q105, q106, q107, q108, q109, q111, q112, q113, q114, q115, q116, q117, q118, q119, q120, q121, q123, q124, q125, q127, q128, q129, q132, q135, q136, q137, q138, q139, q141\"\n",
    "    topics_allowed = topics_allowed.split(\", \")\n",
    "    red_dict = dict()\n",
    "    allow_partially_redundant = 1\n",
    "    for line in open(\"./dataset/apwsj/NoveltyData/redundancy.apwsj.result\", \"r\"):\n",
    "        tokens = line.split()\n",
    "        if tokens[2] == \"?\":\n",
    "            if allow_partially_redundant == 1:\n",
    "                red_dict[tokens[0] + \"/\" + tokens[1]] = [\n",
    "                    tokens[0] + \"/\" + i for i in tokens[3:]\n",
    "                ]\n",
    "        else:\n",
    "            red_dict[tokens[0] + \"/\" + tokens[1]] = [\n",
    "                tokens[0] + \"/\" + i for i in tokens[2:]\n",
    "            ]\n",
    "    crawled = defaultdict(int)\n",
    "    for doc in red_dict:\n",
    "        if crawled[doc] == 0:\n",
    "            red_dict[doc] = crawl(red_dict, doc, crawled)\n",
    "            crawled[doc] = 1\n",
    "    with open(wf, \"w\") as f:\n",
    "        for doc in red_dict:\n",
    "            if doc.split(\"/\")[0] in topics_allowed:\n",
    "                f.write(\n",
    "                    \" \".join(doc.split(\"/\") + [i.split(\"/\")[1] for i in red_dict[doc]])\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "\n",
    "    write_file = \"./dataset/apwsj/novel_list_without_partially_redundant.txt\"\n",
    "    topics = topic_list\n",
    "    doc_topic_dict = defaultdict(list)\n",
    "\n",
    "    for i in topic_doc:\n",
    "        doc_topic_dict[i[0]].append(i[1])\n",
    "    docs_sorted = (\n",
    "        open(\"./dataset/apwsj/NoveltyData/apwsj88-90.rel.docno.sorted\", \"r\")\n",
    "        .read()\n",
    "        .splitlines()\n",
    "    )\n",
    "    sorted_doc_topic_dict = defaultdict(list)\n",
    "    for doc in docs_sorted:\n",
    "        if len(doc_topic_dict[doc]) > 0:\n",
    "            for t in doc_topic_dict[doc]:\n",
    "                sorted_doc_topic_dict[t].append(doc)\n",
    "    redundant_dict = defaultdict(lambda: defaultdict(int))\n",
    "    for line in open(\n",
    "        \"./dataset/apwsj/redundancy_list_without_partially_redundant.txt\", \"r\"\n",
    "    ):\n",
    "        tokens = line.split()\n",
    "        redundant_dict[tokens[0]][tokens[1]] = 1\n",
    "    novel_list = []\n",
    "    for topic in topics:\n",
    "        if topic in topics_allowed:\n",
    "            for i in range(len(sorted_doc_topic_dict[topic])):\n",
    "                if redundant_dict[topic][sorted_doc_topic_dict[topic][i]] != 1:\n",
    "                    if i > 0:\n",
    "                        # take at most 5 latest docs in case of novel\n",
    "                        novel_list.append(\n",
    "                            \" \".join(\n",
    "                                [topic, sorted_doc_topic_dict[topic][i]]\n",
    "                                + sorted_doc_topic_dict[topic][max(0, i - 5) : i]\n",
    "                            )\n",
    "                        )\n",
    "    with open(write_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(novel_list))\n",
    "\n",
    "    # Novel cases\n",
    "    novel_docs = \"./dataset/apwsj/novel_list_without_partially_redundant.txt\"\n",
    "    with open(novel_docs, \"r\") as f:\n",
    "        novel_doc_list = [i.split() for i in f.read().split(\"\\n\")]\n",
    "    # Redundant cases\n",
    "    red_docs = \"./dataset/apwsj/redundancy_list_without_partially_redundant.txt\"\n",
    "    with open(red_docs, \"r\") as f:\n",
    "        red_doc_list = [i.split() for i in f.read().split(\"\\n\")]\n",
    "    red_doc_list = filter(lambda x: len(x) > 0, red_doc_list)\n",
    "    novel_doc_list = filter(lambda x: len(x) > 0, novel_doc_list)\n",
    "\n",
    "    dataset = []\n",
    "    s_not_found = 0\n",
    "    t_not_found = 0\n",
    "    for i in novel_doc_list:\n",
    "        target_id = i[1]\n",
    "        source_ids = i[2:]\n",
    "        if target_id in docs_json.keys():\n",
    "            data_inst = {}\n",
    "            data_inst[\"target\"] = docs_json[target_id]\n",
    "            data_inst[\"source\"] = \"\"\n",
    "            for source_id in source_ids:\n",
    "                if source_id in docs_json.keys():\n",
    "                    data_inst[\"source\"] += docs_json[source_id] + \". \\n\"\n",
    "            data_inst[\"label\"] = 1\n",
    "        else:\n",
    "            print(target_id)\n",
    "        if data_inst[\"source\"] != \"\":\n",
    "            dataset.append(data_inst)\n",
    "\n",
    "    for i in red_doc_list:\n",
    "        target_id = i[1]\n",
    "        source_ids = i[2:]\n",
    "        if target_id in docs_json.keys():\n",
    "            data_inst = {}\n",
    "            data_inst[\"target\"] = docs_json[target_id]\n",
    "            data_inst[\"source\"] = \"\"\n",
    "            for source_id in source_ids:\n",
    "                if source_id in docs_json.keys():\n",
    "                    data_inst[\"source\"] += docs_json[source_id] + \". \\n\"\n",
    "            data_inst[\"label\"] = 0\n",
    "        else:\n",
    "            print(target_id)\n",
    "        if data_inst[\"source\"] != \"\":\n",
    "            dataset.append(data_inst)\n",
    "\n",
    "    dataset_json = {}\n",
    "    for i in range(len(dataset)):\n",
    "        dataset_json[i] = dataset[i]\n",
    "\n",
    "    dataset_path = \"./dataset/apwsj/apwsj_dataset.json\"\n",
    "    with open(dataset_path, \"w\") as f:\n",
    "        json.dump(dataset_json, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('need_these.txt','r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = data.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.unpack_archive('./dataset/dataset_aw.zip','./dataset/ap/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive('./dataset/ap/trec/AP_others.zip','./dataset/ap/trec/AP_others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive('./dataset/ap/trec/AP.tar','./dataset/ap/trec/AP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive('./dataset/ap/trec/split_wsj.zip','./dataset/ap/trec/split_wsj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as osj\n",
    "import glob\n",
    "\n",
    "AP_path = \"./dataset/ap/trec/AP\"\n",
    "\n",
    "\n",
    "AP_files = glob.glob(osj(AP_path,\"*.gz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in AP_files:\n",
    "    with gzip.open(i,'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    with open(i[:-3],'wb') as f_new:\n",
    "        f_new.write(text)\n",
    "    os.remove(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'./dataset/ap/trec/AP/AP881117'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "i[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_files = glob.glob(osj(AP_path,\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./dataset/ap/trec/AP/AP900724\n"
     ]
    }
   ],
   "source": [
    "for i in AP_files:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors= 0 \n",
    "docs_json = {}\n",
    "for ap_file in AP_files:\n",
    "    with open(ap_file, \"r\", encoding=\"latin-1\") as f:\n",
    "        txt = f.read()\n",
    "    docs = [\n",
    "        i.split(\"<DOC>\")[1]\n",
    "        for i in filter(lambda x: len(x) > 10, txt.split(\"</DOC>\"))\n",
    "    ]\n",
    "\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            id = doc.split(\"<DOCNO>\")[1].split(\"</DOCNO>\")[0]\n",
    "            text = doc.split(\"<TEXT>\")[1].split(\"</TEXT>\")[0]\n",
    "            id = id.strip()\n",
    "            docs_json[id] = text\n",
    "        except:\n",
    "            errors += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_found = docs_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = []\n",
    "for i in dat:\n",
    "    if i in new_found:\n",
    "        found.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "len(found)"
   ]
  },
  {
   "source": [
    "!!!!!!!!!!!!!!!!!! FOUND ALL AP !!!!!!!!!!!!!!!!!!!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_split = \"dataset/trec/WSJ/wsj_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "wsj_other_files = []\n",
    "\n",
    "wsj_big = glob.glob(osj(WSJ_split,\"*\"))\n",
    "for i in wsj_big:\n",
    "    for file_path in glob.glob(osj(i,\"*\")):\n",
    "        wsj_other_files.append(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.process_apwsj import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading APWSJ dataset, Errors :  0\n",
      "WSJ890918-0173\n",
      "WSJ880824-0050\n",
      "WSJ890905-0126\n",
      "WSJ890918-0123\n",
      "WSJ880819-0090\n",
      "WSJ880819-0038\n",
      "WSJ890905-0079\n",
      "WSJ880406-0066\n",
      "WSJ880822-0013\n",
      "WSJ880825-0035\n",
      "WSJ880505-0116\n",
      "WSJ880505-0011\n",
      "WSJ880822-0108\n",
      "WSJ880824-0087\n",
      "WSJ880928-0083\n",
      "WSJ890905-0051\n",
      "WSJ890919-0054\n",
      "WSJ890906-0010\n",
      "WSJ880504-0110\n",
      "WSJ880823-0003\n",
      "WSJ880927-0114\n",
      "WSJ890918-0117\n",
      "WSJ880822-0108\n",
      "WSJ880928-0085\n",
      "WSJ880822-0108\n",
      "WSJ890920-0011\n",
      "WSJ880504-0084\n",
      "WSJ880822-0026\n",
      "WSJ880823-0117\n",
      "WSJ880927-0063\n",
      "WSJ880928-0091\n",
      "WSJ890906-0134\n",
      "WSJ890919-0141\n"
     ]
    }
   ],
   "source": [
    "create_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import svn.remote\n",
    "from os.path import join as osj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'process_apwsj' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-48bbd7235678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./dataset/trec/trec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprocess_apwsj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'process_apwsj' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "r = svn.remote.RemoteClient(\n",
    "    \"http://svn.dridan.com/sandpit/QA/trecdata/datacollection/\"\n",
    ")\n",
    "r.checkout(osj(\"./\", \"./dataset/trec/trec\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/NoveltyDetectionResearch/dataset/trec.zip'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"./dataset/trec\", 'zip', \"./dataset/trec/trec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}