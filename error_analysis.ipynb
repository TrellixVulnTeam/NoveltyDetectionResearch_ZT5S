{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.snli import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_dataset_conf = {\"batch_size\":128, \"device\":'cuda','max_len':40,\"tokenizer\":\"bert\"}\n",
    "data = snli(snli_dataset_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "\n",
    "from torchtext.data import Field, Iterator, TabularDataset, BucketIterator, LabelField\n",
    "from torchtext import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "\n",
    "from torchtext.data import Field, Iterator, TabularDataset, BucketIterator, LabelField\n",
    "from torchtext import datasets\n",
    "\n",
    "\n",
    "options = {\"batch_size\":128, \"device\":'cuda','max_len':40,\"tokenizer\":\"bert\"}\n",
    "\n",
    "if options[\"tokenizer\"] == \"bert\":\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    def tokenize_bert(batch):\n",
    "        return tokenizer.encode(batch,add_special_tokens=True,max_length=options['max_len'],padding='max_length')\n",
    "\n",
    "\n",
    "    def tokenize_and_cut(sentence):\n",
    "        tokens = tokenizer.tokenize(sentence) \n",
    "        tokens = tokens[:40-2]\n",
    "        return tokens\n",
    "\n",
    "    sepcial_tokens = tokenizer.special_tokens_map\n",
    "    options[\"init_token\"] = sepcial_tokens[\"cls_token\"]\n",
    "    options[\"pad_token\"] = sepcial_tokens[\"pad_token\"]\n",
    "    options[\"unk_token\"] = sepcial_tokens[\"unk_token\"]\n",
    "    options[\"eos_token\"] = sepcial_tokens[\"sep_token\"]\n",
    "    options[\"use_vocab\"] = False\n",
    "    options[\"tokenize\"] = tokenize_and_cut\n",
    "    options[\"tokenizer\"] = tokenizer\n",
    "\n",
    "if options[\"tokenizer\"] == \"spacy\":\n",
    "        options[\"tokenize\"] = \"spacy\"\n",
    "        options[\"init_token\"] = \"<sos>\"\n",
    "        options[\"unk_token\"] = \"<unk>\"\n",
    "        options[\"pad_token\"] = \"<pad>\"\n",
    "        options[\"eos_token\"] = \"<eos>\"\n",
    "        options[\"use_vocab\"] = True\n",
    "\n",
    "if options.get(\"lower\", None) == None:\n",
    "    options[\"lower\"] = True\n",
    "\n",
    " \n",
    "TEXT = Field(\n",
    "            preprocessing = options[\"tokenizer\"].convert_tokens_to_ids,\n",
    "            batch_first=True,\n",
    "            use_vocab=options[\"use_vocab\"],\n",
    "            tokenize = options[\"tokenize\"],\n",
    "            init_token=options[\"init_token\"],\n",
    "            eos_token=options[\"eos_token\"],\n",
    "            pad_token=options[\"pad_token\"],\n",
    "            unk_token=options[\"unk_token\"],\n",
    "        )\n",
    "LABEL = LabelField(dtype = torch.float)\n",
    "train, dev, test = datasets.SNLI.splits(TEXT, LABEL)\n",
    "\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, dev_iter, test_iter = BucketIterator.splits(\n",
    "            (train, dev, test),\n",
    "            batch_size=options[\"batch_size\"],\n",
    "            device=options[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import LabelField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "print(train_iter.data()[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f79a413a488>, {'entailment': 0, 'contradiction': 1, 'neutral': 2})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'neutral'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train.examples[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "len(tokenizer.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "train_data, valid_data, test_data = datasets.SNLI.splits(TEXT, LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training examples: 549367\nNumber of validation examples: 9842\nNumber of testing examples: 9824\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'premise': [1037, 2879, 2003, 8660, 2006, 17260, 6277, 1999, 1996, 2690, 1997, 1037, 2417, 2958, 1012], 'hypothesis': [1996, 2879, 17260, 2015, 2091, 1996, 11996, 1012], 'label': 'contradiction'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['a', 'boy', 'is', 'jumping', 'on', 'skate', '##board', 'in', 'the', 'middle', 'of', 'a', 'red', 'bridge', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['premise'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f68b8d9a378>, {'entailment': 0, 'contradiction': 1, 'neutral': 2})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n[torchtext.data.batch.Batch of size 128 from SNLI]\n\t[.premise]:[torch.cuda.LongTensor of size 128x32 (GPU 0)]\n\t[.hypothesis]:[torch.cuda.LongTensor of size 128x22 (GPU 0)]\n\t[.label]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n"
     ]
    }
   ],
   "source": [
    "for i in train_iterator:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}